{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58Udxs2T8b1V",
        "outputId": "95e5e982-dd9a-448f-f9bc-991bb1c096e7"
      },
      "source": [
        "!pip install --upgrade pip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 15.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-21.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfQ_Juhh8zw0",
        "outputId": "a7d2c207-8ab6-4916-f8b4-4af88047e3c2"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.10.1\n",
            "  Downloading transformers-4.10.1-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 4.0 MB/s \n",
            "\u001b[?25hCollecting datasets==1.11.0\n",
            "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 70.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (4.62.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.10.0+cu111)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.1->-r requirements.txt (line 1)) (3.4.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 50.5 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 61.4 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.1->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.1->-r requirements.txt (line 1)) (1.19.5)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 639 kB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.1->-r requirements.txt (line 1)) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.1->-r requirements.txt (line 1)) (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.1->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0->-r requirements.txt (line 2)) (3.0.0)\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting fsspec>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 58.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0->-r requirements.txt (line 2)) (0.3.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 73.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0->-r requirements.txt (line 2)) (0.70.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0->-r requirements.txt (line 2)) (1.1.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers==4.10.1->-r requirements.txt (line 1)) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.10.1->-r requirements.txt (line 1)) (3.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.1->-r requirements.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.1->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.1->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.1->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.1->-r requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.11.0->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.11.0->-r requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.11.0->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.1->-r requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.1->-r requirements.txt (line 1)) (1.1.0)\n",
            "Installing collected packages: pyyaml, xxhash, tokenizers, sacremoses, huggingface-hub, fsspec, transformers, datasets\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed datasets-1.11.0 fsspec-2021.11.1 huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.10.1 xxhash-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XowTzgMNRbnp",
        "outputId": "32a8a5ce-37f3-4a48-8f9e-154345352107"
      },
      "source": [
        "#trained on snli\n",
        "!python3 run-conc.py --do_train --task nli --dataset snli --concatenate none --per_device_train_batch_size 60 --output_dir ./trained_model_3/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 3.82kB [00:00, 4.19MB/s]       \n",
            "Downloading: 1.90kB [00:00, 2.47MB/s]     \n",
            "Downloading and preparing dataset snli/plain_text (download: 90.17 MiB, generated: 65.51 MiB, post-processed: Unknown size, total: 155.68 MiB) to /root/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b...\n",
            "Downloading: 100% 1.93k/1.93k [00:00<00:00, 2.33MB/s]\n",
            "Downloading: 100% 1.26M/1.26M [00:00<00:00, 44.0MB/s]\n",
            "Downloading: 100% 65.9M/65.9M [00:01<00:00, 62.0MB/s]\n",
            "Downloading: 100% 1.26M/1.26M [00:00<00:00, 46.6MB/s]\n",
            "Dataset snli downloaded and prepared to /root/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b. Subsequent calls will reuse this data.\n",
            "Downloading: 100% 665/665 [00:00<00:00, 617kB/s]\n",
            "Downloading: 100% 54.2M/54.2M [00:01<00:00, 51.3MB/s]\n",
            "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 27.6kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 320kB/s]\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 516kB/s]\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "100% 10/10 [00:00<00:00, 37.82ba/s]\n",
            "100% 551/551 [00:14<00:00, 39.12ba/s]\n",
            "100% 10/10 [00:00<00:00, 40.11ba/s]\n",
            " #0:   0% 0/275 [00:00<?, ?ba/s]\n",
            " #0:   0% 1/275 [00:00<00:52,  5.24ba/s]\n",
            " #0:   1% 2/275 [00:00<00:49,  5.57ba/s]\n",
            " #0:   1% 3/275 [00:00<00:48,  5.65ba/s]\n",
            " #0:   1% 4/275 [00:00<00:48,  5.56ba/s]\n",
            " #0:   2% 5/275 [00:00<00:47,  5.66ba/s]\n",
            " #0:   2% 6/275 [00:01<01:09,  3.89ba/s]\n",
            " #0:   3% 7/275 [00:01<01:02,  4.28ba/s]\n",
            " #0:   3% 8/275 [00:01<00:58,  4.57ba/s]\n",
            " #0:   3% 9/275 [00:01<00:54,  4.92ba/s]\n",
            " #0:   4% 10/275 [00:02<00:50,  5.21ba/s]\n",
            " #0:   4% 11/275 [00:02<00:48,  5.39ba/s]\n",
            " #0:   4% 12/275 [00:02<00:47,  5.50ba/s]\n",
            " #0:   5% 13/275 [00:02<00:47,  5.51ba/s]\n",
            " #0:   5% 14/275 [00:02<00:46,  5.59ba/s]\n",
            " #0:   5% 15/275 [00:02<00:45,  5.66ba/s]\n",
            " #0:   6% 16/275 [00:03<00:45,  5.75ba/s]\n",
            " #0:   6% 17/275 [00:03<00:44,  5.85ba/s]\n",
            " #0:   7% 18/275 [00:03<00:43,  5.92ba/s]\n",
            " #0:   7% 19/275 [00:03<00:43,  5.86ba/s]\n",
            " #0:   7% 20/275 [00:03<00:43,  5.88ba/s]\n",
            " #0:   8% 21/275 [00:03<00:43,  5.91ba/s]\n",
            " #0:   8% 22/275 [00:04<00:43,  5.84ba/s]\n",
            " #0:   8% 23/275 [00:04<00:43,  5.80ba/s]\n",
            " #0:   9% 24/275 [00:04<00:42,  5.92ba/s]\n",
            " #0:   9% 25/275 [00:04<00:42,  5.87ba/s]\n",
            " #0:   9% 26/275 [00:04<00:42,  5.92ba/s]\n",
            " #0:  10% 27/275 [00:04<00:42,  5.78ba/s]\n",
            " #0:  10% 28/275 [00:05<00:42,  5.77ba/s]\n",
            " #0:  11% 29/275 [00:05<00:53,  4.56ba/s]\n",
            " #0:  11% 30/275 [00:05<00:50,  4.82ba/s]\n",
            " #0:  11% 31/275 [00:05<00:48,  5.07ba/s]\n",
            " #0:  12% 32/275 [00:05<00:46,  5.26ba/s]\n",
            " #0:  12% 33/275 [00:06<00:45,  5.31ba/s]\n",
            " #0:  12% 34/275 [00:06<00:44,  5.39ba/s]\n",
            " #0:  13% 35/275 [00:06<00:43,  5.50ba/s]\n",
            " #0:  13% 36/275 [00:06<00:42,  5.58ba/s]\n",
            " #0:  13% 37/275 [00:06<00:42,  5.58ba/s]\n",
            " #0:  14% 38/275 [00:07<00:42,  5.57ba/s]\n",
            " #0:  14% 39/275 [00:07<00:41,  5.66ba/s]\n",
            " #0:  15% 40/275 [00:07<00:41,  5.64ba/s]\n",
            " #0:  15% 41/275 [00:07<00:41,  5.57ba/s]\n",
            " #0:  15% 42/275 [00:07<00:41,  5.58ba/s]\n",
            " #0:  16% 43/275 [00:07<00:41,  5.64ba/s]\n",
            " #1:  16% 43/275 [00:07<00:39,  5.84ba/s]\u001b[A\n",
            " #0:  16% 44/275 [00:08<00:41,  5.61ba/s]\n",
            " #0:  16% 45/275 [00:08<00:41,  5.61ba/s]\n",
            " #0:  17% 47/275 [00:08<00:40,  5.67ba/s]\n",
            " #0:  17% 48/275 [00:08<00:41,  5.51ba/s]\n",
            " #1:  17% 48/275 [00:08<00:40,  5.55ba/s]\u001b[A\n",
            " #0:  18% 49/275 [00:09<00:42,  5.36ba/s]\n",
            " #0:  18% 50/275 [00:09<00:41,  5.45ba/s]\n",
            " #0:  19% 51/275 [00:09<00:40,  5.50ba/s]\n",
            " #0:  19% 52/275 [00:09<00:51,  4.32ba/s]\n",
            " #0:  19% 53/275 [00:09<00:47,  4.70ba/s]\n",
            " #0:  20% 54/275 [00:10<00:48,  4.58ba/s]\n",
            " #0:  20% 55/275 [00:10<00:45,  4.85ba/s]\n",
            " #0:  20% 56/275 [00:10<00:42,  5.13ba/s]\n",
            " #1:  21% 57/275 [00:10<00:39,  5.54ba/s]\u001b[A\n",
            " #0:  21% 57/275 [00:10<00:49,  4.43ba/s]\n",
            " #0:  21% 58/275 [00:10<00:45,  4.73ba/s]\n",
            " #0:  21% 59/275 [00:11<00:43,  4.98ba/s]\n",
            " #0:  22% 60/275 [00:11<00:41,  5.24ba/s]\n",
            " #0:  22% 61/275 [00:11<00:41,  5.21ba/s]\n",
            " #0:  23% 62/275 [00:11<00:40,  5.29ba/s]\n",
            " #0:  23% 63/275 [00:11<00:38,  5.47ba/s]\n",
            " #0:  23% 64/275 [00:11<00:37,  5.61ba/s]\n",
            " #0:  24% 65/275 [00:12<00:36,  5.70ba/s]\n",
            " #0:  24% 66/275 [00:12<00:36,  5.75ba/s]\n",
            " #0:  24% 67/275 [00:12<00:35,  5.79ba/s]\n",
            " #0:  25% 68/275 [00:12<00:36,  5.68ba/s]\n",
            " #0:  25% 69/275 [00:12<00:35,  5.74ba/s]\n",
            " #0:  25% 70/275 [00:13<00:35,  5.71ba/s]\n",
            " #0:  26% 71/275 [00:13<00:36,  5.66ba/s]\n",
            " #0:  26% 72/275 [00:13<00:35,  5.78ba/s]\n",
            " #0:  27% 74/275 [00:13<00:34,  5.86ba/s]\n",
            " #0:  27% 75/275 [00:14<00:42,  4.72ba/s]\n",
            " #0:  28% 76/275 [00:14<00:39,  4.99ba/s]\n",
            " #0:  28% 77/275 [00:14<00:37,  5.24ba/s]\n",
            " #0:  28% 78/275 [00:14<00:37,  5.30ba/s]\n",
            " #1:  29% 79/275 [00:14<00:36,  5.40ba/s]\u001b[A\n",
            " #0:  29% 80/275 [00:14<00:35,  5.45ba/s]\n",
            " #1:  29% 81/275 [00:14<00:34,  5.56ba/s]\u001b[A\n",
            " #0:  29% 81/275 [00:15<00:35,  5.52ba/s]\n",
            " #0:  30% 82/275 [00:15<00:37,  5.09ba/s]\n",
            " #0:  30% 83/275 [00:15<00:36,  5.32ba/s]\n",
            " #0:  31% 84/275 [00:15<00:34,  5.50ba/s]\n",
            " #0:  31% 85/275 [00:15<00:34,  5.46ba/s]\n",
            " #0:  31% 86/275 [00:16<00:35,  5.35ba/s]\n",
            " #0:  32% 87/275 [00:16<00:34,  5.50ba/s]\n",
            " #0:  32% 88/275 [00:16<00:33,  5.66ba/s]\n",
            " #0:  32% 89/275 [00:16<00:32,  5.72ba/s]\n",
            " #0:  33% 90/275 [00:16<00:31,  5.79ba/s]\n",
            " #0:  33% 91/275 [00:16<00:31,  5.80ba/s]\n",
            " #0:  33% 92/275 [00:17<00:31,  5.86ba/s]\n",
            " #0:  34% 93/275 [00:17<00:30,  5.88ba/s]\n",
            " #0:  34% 94/275 [00:17<00:30,  5.86ba/s]\n",
            " #0:  35% 96/275 [00:17<00:30,  5.78ba/s]\n",
            " #0:  35% 97/275 [00:17<00:31,  5.71ba/s]\n",
            " #0:  36% 98/275 [00:18<00:38,  4.60ba/s]\n",
            " #0:  36% 99/275 [00:18<00:36,  4.87ba/s]\n",
            " #0:  36% 100/275 [00:18<00:34,  5.09ba/s]\n",
            " #0:  37% 101/275 [00:18<00:32,  5.30ba/s]\n",
            " #0:  37% 102/275 [00:18<00:32,  5.34ba/s]\n",
            " #0:  37% 103/275 [00:19<00:31,  5.51ba/s]\n",
            " #0:  38% 104/275 [00:19<00:30,  5.62ba/s]\n",
            " #0:  38% 105/275 [00:19<00:29,  5.71ba/s]\n",
            " #0:  39% 106/275 [00:19<00:29,  5.73ba/s]\n",
            " #0:  39% 107/275 [00:19<00:29,  5.67ba/s]\n",
            " #0:  39% 108/275 [00:19<00:29,  5.71ba/s]\n",
            " #0:  40% 109/275 [00:20<00:29,  5.70ba/s]\n",
            " #1:  40% 110/275 [00:20<00:28,  5.71ba/s]\u001b[A\n",
            " #0:  40% 110/275 [00:20<00:30,  5.39ba/s]\n",
            " #0:  41% 112/275 [00:20<00:28,  5.64ba/s]\n",
            " #1:  41% 113/275 [00:20<00:28,  5.60ba/s]\u001b[A\n",
            " #0:  41% 113/275 [00:20<00:28,  5.62ba/s]\n",
            " #0:  41% 114/275 [00:21<00:29,  5.47ba/s]\n",
            " #0:  42% 115/275 [00:21<00:28,  5.60ba/s]\n",
            " #0:  43% 117/275 [00:21<00:27,  5.68ba/s]\n",
            " #1:  43% 118/275 [00:21<00:28,  5.58ba/s]\u001b[A\n",
            " #0:  43% 118/275 [00:21<00:27,  5.76ba/s]\n",
            " #0:  44% 120/275 [00:22<00:27,  5.67ba/s]\n",
            " #1:  44% 121/275 [00:22<00:33,  4.54ba/s]\u001b[A\n",
            " #0:  44% 121/275 [00:22<00:33,  4.53ba/s]\n",
            " #0:  44% 122/275 [00:22<00:31,  4.82ba/s]\n",
            " #0:  45% 123/275 [00:22<00:29,  5.11ba/s]\n",
            " #0:  45% 124/275 [00:22<00:28,  5.25ba/s]\n",
            " #0:  45% 125/275 [00:23<00:27,  5.36ba/s]\n",
            " #0:  46% 126/275 [00:23<00:26,  5.54ba/s]\n",
            " #0:  47% 128/275 [00:23<00:25,  5.70ba/s]\n",
            " #0:  47% 129/275 [00:23<00:25,  5.74ba/s]\n",
            " #0:  47% 130/275 [00:23<00:25,  5.69ba/s]\n",
            " #0:  48% 131/275 [00:24<00:25,  5.76ba/s]\n",
            " #0:  48% 132/275 [00:24<00:24,  5.75ba/s]\n",
            " #0:  48% 133/275 [00:24<00:24,  5.81ba/s]\n",
            " #0:  49% 134/275 [00:24<00:24,  5.77ba/s]\n",
            " #1:  49% 135/275 [00:24<00:24,  5.81ba/s]\u001b[A\n",
            " #0:  49% 136/275 [00:25<00:23,  5.81ba/s]\n",
            " #0:  50% 137/275 [00:25<00:23,  5.79ba/s]\n",
            " #0:  50% 138/275 [00:25<00:23,  5.79ba/s]\n",
            " #0:  51% 139/275 [00:25<00:23,  5.68ba/s]\n",
            " #0:  51% 140/275 [00:25<00:23,  5.72ba/s]\n",
            " #0:  51% 141/275 [00:25<00:23,  5.78ba/s]\n",
            " #0:  52% 142/275 [00:26<00:22,  5.81ba/s]\n",
            " #0:  52% 143/275 [00:26<00:22,  5.82ba/s]\n",
            " #0:  52% 144/275 [00:26<00:28,  4.65ba/s]\n",
            " #0:  53% 145/275 [00:26<00:25,  5.02ba/s]\n",
            " #0:  53% 146/275 [00:26<00:24,  5.22ba/s]\n",
            " #0:  53% 147/275 [00:27<00:24,  5.21ba/s]\n",
            " #0:  54% 148/275 [00:27<00:23,  5.36ba/s]\n",
            " #0:  54% 149/275 [00:27<00:22,  5.53ba/s]\n",
            " #0:  55% 150/275 [00:27<00:21,  5.69ba/s]\n",
            " #0:  55% 151/275 [00:27<00:22,  5.63ba/s]\n",
            " #0:  55% 152/275 [00:27<00:21,  5.71ba/s]\n",
            " #0:  56% 153/275 [00:28<00:21,  5.66ba/s]\n",
            " #0:  56% 154/275 [00:28<00:21,  5.72ba/s]\n",
            " #0:  56% 155/275 [00:28<00:20,  5.85ba/s]\n",
            " #0:  57% 156/275 [00:28<00:20,  5.89ba/s]\n",
            " #0:  57% 157/275 [00:28<00:19,  5.98ba/s]\n",
            " #0:  57% 158/275 [00:28<00:19,  5.97ba/s]\n",
            " #0:  58% 159/275 [00:29<00:19,  5.86ba/s]\n",
            " #0:  58% 160/275 [00:29<00:19,  5.83ba/s]\n",
            " #0:  59% 161/275 [00:29<00:19,  5.84ba/s]\n",
            " #0:  59% 162/275 [00:29<00:19,  5.88ba/s]\n",
            " #0:  59% 163/275 [00:29<00:18,  5.93ba/s]\n",
            " #0:  60% 164/275 [00:29<00:18,  5.95ba/s]\n",
            " #0:  60% 165/275 [00:30<00:18,  5.88ba/s]\n",
            " #0:  60% 166/275 [00:30<00:18,  5.82ba/s]\n",
            " #0:  61% 167/275 [00:30<00:24,  4.35ba/s]\n",
            " #0:  61% 168/275 [00:30<00:22,  4.68ba/s]\n",
            " #0:  61% 169/275 [00:31<00:21,  4.87ba/s]\n",
            " #0:  62% 170/275 [00:31<00:21,  4.97ba/s]\n",
            " #0:  62% 171/275 [00:31<00:20,  5.17ba/s]\n",
            " #0:  63% 172/275 [00:31<00:19,  5.35ba/s]\n",
            " #0:  63% 173/275 [00:31<00:18,  5.45ba/s]\n",
            " #0:  63% 174/275 [00:31<00:18,  5.61ba/s]\n",
            " #0:  64% 175/275 [00:32<00:17,  5.63ba/s]\n",
            " #0:  64% 176/275 [00:32<00:17,  5.71ba/s]\n",
            " #0:  64% 177/275 [00:32<00:17,  5.66ba/s]\n",
            " #0:  65% 178/275 [00:32<00:16,  5.74ba/s]\n",
            " #0:  65% 179/275 [00:32<00:16,  5.76ba/s]\n",
            " #0:  65% 180/275 [00:32<00:16,  5.87ba/s]\n",
            " #0:  66% 181/275 [00:33<00:16,  5.72ba/s]\n",
            " #0:  66% 182/275 [00:33<00:16,  5.81ba/s]\n",
            " #0:  67% 183/275 [00:33<00:15,  5.79ba/s]\n",
            " #0:  67% 184/275 [00:33<00:15,  5.89ba/s]\n",
            " #0:  67% 185/275 [00:33<00:15,  5.85ba/s]\n",
            " #0:  68% 186/275 [00:33<00:15,  5.91ba/s]\n",
            " #0:  68% 187/275 [00:34<00:15,  5.87ba/s]\n",
            " #0:  68% 188/275 [00:34<00:14,  5.96ba/s]\n",
            " #0:  69% 189/275 [00:34<00:14,  5.93ba/s]\n",
            " #0:  69% 190/275 [00:34<00:17,  4.80ba/s]\n",
            " #0:  69% 191/275 [00:34<00:16,  5.08ba/s]\n",
            " #0:  70% 192/275 [00:35<00:15,  5.23ba/s]\n",
            " #0:  70% 193/275 [00:35<00:14,  5.47ba/s]\n",
            " #0:  71% 194/275 [00:35<00:14,  5.56ba/s]\n",
            " #0:  71% 195/275 [00:35<00:15,  5.28ba/s]\n",
            " #0:  71% 196/275 [00:35<00:15,  5.25ba/s]\n",
            " #0:  72% 197/275 [00:36<00:14,  5.35ba/s]\n",
            " #0:  72% 198/275 [00:36<00:14,  5.43ba/s]\n",
            " #0:  72% 199/275 [00:36<00:13,  5.53ba/s]\n",
            " #0:  73% 200/275 [00:36<00:13,  5.73ba/s]\n",
            " #0:  73% 201/275 [00:36<00:12,  5.77ba/s]\n",
            " #0:  73% 202/275 [00:36<00:12,  5.84ba/s]\n",
            " #0:  74% 203/275 [00:37<00:12,  5.86ba/s]\n",
            " #0:  74% 204/275 [00:37<00:12,  5.81ba/s]\n",
            " #0:  75% 205/275 [00:37<00:12,  5.78ba/s]\n",
            " #0:  75% 206/275 [00:37<00:11,  5.78ba/s]\n",
            " #0:  75% 207/275 [00:37<00:11,  5.84ba/s]\n",
            " #0:  76% 208/275 [00:37<00:11,  5.87ba/s]\n",
            " #0:  76% 209/275 [00:38<00:11,  5.92ba/s]\n",
            " #0:  76% 210/275 [00:38<00:11,  5.88ba/s]\n",
            " #0:  77% 211/275 [00:38<00:10,  5.88ba/s]\n",
            " #0:  77% 212/275 [00:38<00:10,  5.81ba/s]\n",
            " #0:  77% 213/275 [00:38<00:13,  4.61ba/s]\n",
            " #0:  78% 214/275 [00:39<00:12,  4.95ba/s]\n",
            " #0:  78% 215/275 [00:39<00:11,  5.12ba/s]\n",
            " #0:  79% 216/275 [00:39<00:11,  5.35ba/s]\n",
            " #0:  79% 217/275 [00:39<00:10,  5.45ba/s]\n",
            " #0:  79% 218/275 [00:39<00:10,  5.65ba/s]\n",
            " #0:  80% 219/275 [00:39<00:09,  5.72ba/s]\n",
            " #0:  80% 220/275 [00:40<00:09,  5.83ba/s]\n",
            " #0:  80% 221/275 [00:40<00:09,  5.75ba/s]\n",
            " #0:  81% 222/275 [00:40<00:09,  5.89ba/s]\n",
            " #0:  81% 223/275 [00:40<00:08,  5.89ba/s]\n",
            " #0:  81% 224/275 [00:40<00:08,  5.92ba/s]\n",
            " #0:  82% 225/275 [00:40<00:08,  5.91ba/s]\n",
            " #0:  83% 227/275 [00:41<00:08,  5.79ba/s]\n",
            " #0:  83% 228/275 [00:41<00:08,  5.84ba/s]\n",
            " #1:  83% 228/275 [00:41<00:08,  5.71ba/s]\u001b[A\n",
            " #0:  84% 230/275 [00:41<00:08,  5.60ba/s]\n",
            " #0:  84% 231/275 [00:42<00:07,  5.62ba/s]\n",
            " #1:  84% 231/275 [00:41<00:07,  5.59ba/s]\u001b[A\n",
            " #0:  84% 232/275 [00:42<00:07,  5.69ba/s]\n",
            " #0:  85% 233/275 [00:42<00:07,  5.43ba/s]\n",
            " #0:  85% 234/275 [00:42<00:07,  5.56ba/s]\n",
            " #0:  85% 235/275 [00:42<00:07,  5.60ba/s]\n",
            " #0:  86% 236/275 [00:43<00:08,  4.42ba/s]\n",
            " #0:  86% 237/275 [00:43<00:08,  4.67ba/s]\n",
            " #0:  87% 238/275 [00:43<00:07,  4.82ba/s]\n",
            " #0:  87% 239/275 [00:43<00:07,  5.05ba/s]\n",
            " #0:  88% 241/275 [00:43<00:06,  5.45ba/s]\n",
            " #1:  88% 241/275 [00:43<00:06,  5.34ba/s]\u001b[A\n",
            " #0:  88% 242/275 [00:44<00:05,  5.52ba/s]\n",
            " #0:  89% 244/275 [00:44<00:05,  5.65ba/s]\n",
            " #1:  89% 244/275 [00:44<00:05,  5.58ba/s]\u001b[A\n",
            " #0:  89% 245/275 [00:44<00:05,  5.70ba/s]\n",
            " #0:  89% 246/275 [00:44<00:05,  5.74ba/s]\n",
            " #0:  90% 247/275 [00:44<00:04,  5.88ba/s]\n",
            " #0:  91% 249/275 [00:45<00:04,  5.88ba/s]\n",
            " #0:  91% 250/275 [00:45<00:04,  5.84ba/s]\n",
            " #0:  91% 251/275 [00:45<00:04,  5.92ba/s]\n",
            " #1:  91% 251/275 [00:45<00:04,  5.85ba/s]\u001b[A\n",
            " #0:  92% 252/275 [00:45<00:04,  5.62ba/s]\n",
            " #0:  92% 253/275 [00:46<00:04,  5.40ba/s]\n",
            " #0:  92% 254/275 [00:46<00:04,  5.19ba/s]\n",
            " #0:  93% 255/275 [00:46<00:03,  5.25ba/s]\n",
            " #0:  93% 256/275 [00:46<00:03,  5.46ba/s]\n",
            " #0:  93% 257/275 [00:46<00:03,  5.52ba/s]\n",
            " #0:  94% 258/275 [00:46<00:02,  5.70ba/s]\n",
            " #0:  94% 259/275 [00:47<00:03,  4.47ba/s]\n",
            " #0:  95% 260/275 [00:47<00:03,  4.77ba/s]\n",
            " #0:  95% 261/275 [00:47<00:02,  5.08ba/s]\n",
            " #0:  95% 262/275 [00:47<00:02,  5.19ba/s]\n",
            " #0:  96% 263/275 [00:48<00:02,  5.41ba/s]\n",
            " #0:  96% 264/275 [00:48<00:01,  5.53ba/s]\n",
            " #0:  96% 265/275 [00:48<00:01,  5.67ba/s]\n",
            " #0:  97% 266/275 [00:48<00:01,  5.61ba/s]\n",
            " #0:  97% 267/275 [00:48<00:01,  5.79ba/s]\n",
            " #0:  97% 268/275 [00:48<00:01,  5.84ba/s]\n",
            " #0:  98% 269/275 [00:49<00:01,  5.90ba/s]\n",
            " #0:  98% 270/275 [00:49<00:00,  5.92ba/s]\n",
            " #0:  99% 271/275 [00:49<00:00,  5.94ba/s]\n",
            " #0:  99% 272/275 [00:49<00:00,  5.81ba/s]\n",
            " #0:  99% 273/275 [00:49<00:00,  5.72ba/s]\n",
            " #0: 100% 274/275 [00:49<00:00,  5.74ba/s]\n",
            " #1: 100% 275/275 [00:49<00:00,  5.51ba/s]\n",
            " #0: 100% 275/275 [00:50<00:00,  5.50ba/s]\n",
            "***** Running training *****\n",
            "  Num examples = 549367\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 60\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 60\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 27471\n",
            "{'loss': 0.7459, 'learning_rate': 4.9089949401186706e-05, 'epoch': 0.05}\n",
            "  2% 500/27471 [01:25<1:16:28,  5.88it/s]Saving model checkpoint to ./trained_model_3/checkpoint-500\n",
            "Configuration saved in ./trained_model_3/checkpoint-500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.5544, 'learning_rate': 4.817989880237341e-05, 'epoch': 0.11}\n",
            "  4% 1000/27471 [02:51<1:15:34,  5.84it/s]Saving model checkpoint to ./trained_model_3/checkpoint-1000\n",
            "Configuration saved in ./trained_model_3/checkpoint-1000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.511, 'learning_rate': 4.726984820356012e-05, 'epoch': 0.16}\n",
            "  5% 1500/27471 [04:16<1:13:34,  5.88it/s]Saving model checkpoint to ./trained_model_3/checkpoint-1500\n",
            "Configuration saved in ./trained_model_3/checkpoint-1500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 0.4885, 'learning_rate': 4.6359797604746824e-05, 'epoch': 0.22}\n",
            "  7% 2000/27471 [05:42<1:12:28,  5.86it/s]Saving model checkpoint to ./trained_model_3/checkpoint-2000\n",
            "Configuration saved in ./trained_model_3/checkpoint-2000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 0.464, 'learning_rate': 4.544974700593353e-05, 'epoch': 0.27}\n",
            "  9% 2500/27471 [07:08<1:10:53,  5.87it/s]Saving model checkpoint to ./trained_model_3/checkpoint-2500\n",
            "Configuration saved in ./trained_model_3/checkpoint-2500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 0.4521, 'learning_rate': 4.453969640712024e-05, 'epoch': 0.33}\n",
            " 11% 3000/27471 [08:33<1:09:56,  5.83it/s]Saving model checkpoint to ./trained_model_3/checkpoint-3000\n",
            "Configuration saved in ./trained_model_3/checkpoint-3000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 0.4461, 'learning_rate': 4.362964580830694e-05, 'epoch': 0.38}\n",
            " 13% 3500/27471 [09:59<1:07:41,  5.90it/s]Saving model checkpoint to ./trained_model_3/checkpoint-3500\n",
            "Configuration saved in ./trained_model_3/checkpoint-3500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 0.4434, 'learning_rate': 4.271959520949365e-05, 'epoch': 0.44}\n",
            " 15% 4000/27471 [11:25<1:06:32,  5.88it/s]Saving model checkpoint to ./trained_model_3/checkpoint-4000\n",
            "Configuration saved in ./trained_model_3/checkpoint-4000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 0.4301, 'learning_rate': 4.180954461068036e-05, 'epoch': 0.49}\n",
            " 16% 4500/27471 [12:51<1:05:05,  5.88it/s]Saving model checkpoint to ./trained_model_3/checkpoint-4500\n",
            "Configuration saved in ./trained_model_3/checkpoint-4500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-4500/special_tokens_map.json\n",
            "{'loss': 0.4268, 'learning_rate': 4.089949401186706e-05, 'epoch': 0.55}\n",
            " 18% 5000/27471 [14:16<1:03:36,  5.89it/s]Saving model checkpoint to ./trained_model_3/checkpoint-5000\n",
            "Configuration saved in ./trained_model_3/checkpoint-5000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-5000/special_tokens_map.json\n",
            "{'loss': 0.4169, 'learning_rate': 3.9989443413053765e-05, 'epoch': 0.6}\n",
            " 20% 5500/27471 [15:42<1:02:00,  5.91it/s]Saving model checkpoint to ./trained_model_3/checkpoint-5500\n",
            "Configuration saved in ./trained_model_3/checkpoint-5500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-5500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-5500/special_tokens_map.json\n",
            "{'loss': 0.4085, 'learning_rate': 3.9079392814240475e-05, 'epoch': 0.66}\n",
            " 22% 6000/27471 [17:08<1:00:46,  5.89it/s]Saving model checkpoint to ./trained_model_3/checkpoint-6000\n",
            "Configuration saved in ./trained_model_3/checkpoint-6000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-6000/special_tokens_map.json\n",
            "{'loss': 0.4076, 'learning_rate': 3.816934221542718e-05, 'epoch': 0.71}\n",
            " 24% 6500/27471 [18:33<59:20,  5.89it/s]Saving model checkpoint to ./trained_model_3/checkpoint-6500\n",
            "Configuration saved in ./trained_model_3/checkpoint-6500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-6500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-6500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-6500/special_tokens_map.json\n",
            "{'loss': 0.396, 'learning_rate': 3.725929161661388e-05, 'epoch': 0.76}\n",
            " 25% 7000/27471 [19:59<57:52,  5.90it/s]Saving model checkpoint to ./trained_model_3/checkpoint-7000\n",
            "Configuration saved in ./trained_model_3/checkpoint-7000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-7000/special_tokens_map.json\n",
            "{'loss': 0.3994, 'learning_rate': 3.634924101780059e-05, 'epoch': 0.82}\n",
            " 27% 7500/27471 [21:25<56:24,  5.90it/s]Saving model checkpoint to ./trained_model_3/checkpoint-7500\n",
            "Configuration saved in ./trained_model_3/checkpoint-7500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-7500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-7500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-7500/special_tokens_map.json\n",
            "{'loss': 0.3949, 'learning_rate': 3.54391904189873e-05, 'epoch': 0.87}\n",
            " 29% 8000/27471 [22:50<55:34,  5.84it/s]Saving model checkpoint to ./trained_model_3/checkpoint-8000\n",
            "Configuration saved in ./trained_model_3/checkpoint-8000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-8000/special_tokens_map.json\n",
            "{'loss': 0.3874, 'learning_rate': 3.452913982017401e-05, 'epoch': 0.93}\n",
            " 31% 8500/27471 [24:16<53:53,  5.87it/s]Saving model checkpoint to ./trained_model_3/checkpoint-8500\n",
            "Configuration saved in ./trained_model_3/checkpoint-8500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-8500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-8500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-8500/special_tokens_map.json\n",
            "{'loss': 0.3834, 'learning_rate': 3.361908922136071e-05, 'epoch': 0.98}\n",
            " 33% 9000/27471 [25:42<52:19,  5.88it/s]Saving model checkpoint to ./trained_model_3/checkpoint-9000\n",
            "Configuration saved in ./trained_model_3/checkpoint-9000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-9000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-9000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-9000/special_tokens_map.json\n",
            "{'loss': 0.365, 'learning_rate': 3.2709038622547416e-05, 'epoch': 1.04}\n",
            " 35% 9500/27471 [27:07<51:02,  5.87it/s]Saving model checkpoint to ./trained_model_3/checkpoint-9500\n",
            "Configuration saved in ./trained_model_3/checkpoint-9500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-9500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-9500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-9500/special_tokens_map.json\n",
            "{'loss': 0.3465, 'learning_rate': 3.179898802373412e-05, 'epoch': 1.09}\n",
            " 36% 10000/27471 [28:33<49:24,  5.89it/s]Saving model checkpoint to ./trained_model_3/checkpoint-10000\n",
            "Configuration saved in ./trained_model_3/checkpoint-10000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-10000/special_tokens_map.json\n",
            "{'loss': 0.3589, 'learning_rate': 3.088893742492083e-05, 'epoch': 1.15}\n",
            " 38% 10500/27471 [29:58<48:01,  5.89it/s]Saving model checkpoint to ./trained_model_3/checkpoint-10500\n",
            "Configuration saved in ./trained_model_3/checkpoint-10500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-10500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-10500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-10500/special_tokens_map.json\n",
            "{'loss': 0.3508, 'learning_rate': 2.9978886826107534e-05, 'epoch': 1.2}\n",
            " 40% 11000/27471 [31:24<46:39,  5.88it/s]Saving model checkpoint to ./trained_model_3/checkpoint-11000\n",
            "Configuration saved in ./trained_model_3/checkpoint-11000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-11000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-11000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-11000/special_tokens_map.json\n",
            "{'loss': 0.3413, 'learning_rate': 2.9068836227294238e-05, 'epoch': 1.26}\n",
            " 42% 11500/27471 [32:50<45:06,  5.90it/s]Saving model checkpoint to ./trained_model_3/checkpoint-11500\n",
            "Configuration saved in ./trained_model_3/checkpoint-11500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-11500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-11500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-11500/special_tokens_map.json\n",
            "{'loss': 0.3494, 'learning_rate': 2.8158785628480945e-05, 'epoch': 1.31}\n",
            " 44% 12000/27471 [34:15<43:55,  5.87it/s]Saving model checkpoint to ./trained_model_3/checkpoint-12000\n",
            "Configuration saved in ./trained_model_3/checkpoint-12000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-12000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-12000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-12000/special_tokens_map.json\n",
            "{'loss': 0.3409, 'learning_rate': 2.724873502966765e-05, 'epoch': 1.37}\n",
            " 46% 12500/27471 [35:41<42:10,  5.92it/s]Saving model checkpoint to ./trained_model_3/checkpoint-12500\n",
            "Configuration saved in ./trained_model_3/checkpoint-12500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-12500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-12500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-12500/special_tokens_map.json\n",
            "{'loss': 0.3477, 'learning_rate': 2.6338684430854356e-05, 'epoch': 1.42}\n",
            " 47% 13000/27471 [37:06<41:26,  5.82it/s]Saving model checkpoint to ./trained_model_3/checkpoint-13000\n",
            "Configuration saved in ./trained_model_3/checkpoint-13000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-13000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-13000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-13000/special_tokens_map.json\n",
            "{'loss': 0.3474, 'learning_rate': 2.542863383204106e-05, 'epoch': 1.47}\n",
            " 49% 13500/27471 [38:32<39:26,  5.90it/s]Saving model checkpoint to ./trained_model_3/checkpoint-13500\n",
            "Configuration saved in ./trained_model_3/checkpoint-13500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-13500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-13500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-13500/special_tokens_map.json\n",
            "{'loss': 0.3404, 'learning_rate': 2.4518583233227767e-05, 'epoch': 1.53}\n",
            " 51% 14000/27471 [39:57<38:07,  5.89it/s]Saving model checkpoint to ./trained_model_3/checkpoint-14000\n",
            "Configuration saved in ./trained_model_3/checkpoint-14000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-14000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-14000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-14000/special_tokens_map.json\n",
            "{'loss': 0.3375, 'learning_rate': 2.3608532634414474e-05, 'epoch': 1.58}\n",
            " 53% 14500/27471 [41:23<36:47,  5.88it/s]Saving model checkpoint to ./trained_model_3/checkpoint-14500\n",
            "Configuration saved in ./trained_model_3/checkpoint-14500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-14500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-14500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-14500/special_tokens_map.json\n",
            "{'loss': 0.3386, 'learning_rate': 2.269848203560118e-05, 'epoch': 1.64}\n",
            " 55% 15000/27471 [42:49<35:48,  5.80it/s]Saving model checkpoint to ./trained_model_3/checkpoint-15000\n",
            "Configuration saved in ./trained_model_3/checkpoint-15000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-15000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-15000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-15000/special_tokens_map.json\n",
            "{'loss': 0.3425, 'learning_rate': 2.178843143678789e-05, 'epoch': 1.69}\n",
            " 56% 15500/27471 [44:14<33:56,  5.88it/s]Saving model checkpoint to ./trained_model_3/checkpoint-15500\n",
            "Configuration saved in ./trained_model_3/checkpoint-15500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-15500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-15500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-15500/special_tokens_map.json\n",
            "{'loss': 0.3427, 'learning_rate': 2.0878380837974592e-05, 'epoch': 1.75}\n",
            " 58% 16000/27471 [45:40<32:34,  5.87it/s]Saving model checkpoint to ./trained_model_3/checkpoint-16000\n",
            "Configuration saved in ./trained_model_3/checkpoint-16000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-16000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-16000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-16000/special_tokens_map.json\n",
            "{'loss': 0.3295, 'learning_rate': 1.99683302391613e-05, 'epoch': 1.8}\n",
            " 60% 16500/27471 [47:06<31:00,  5.90it/s]Saving model checkpoint to ./trained_model_3/checkpoint-16500\n",
            "Configuration saved in ./trained_model_3/checkpoint-16500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-16500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-16500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-16500/special_tokens_map.json\n",
            "{'loss': 0.3336, 'learning_rate': 1.9058279640348003e-05, 'epoch': 1.86}\n",
            " 62% 17000/27471 [48:31<29:54,  5.84it/s]Saving model checkpoint to ./trained_model_3/checkpoint-17000\n",
            "Configuration saved in ./trained_model_3/checkpoint-17000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-17000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-17000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-17000/special_tokens_map.json\n",
            "{'loss': 0.3382, 'learning_rate': 1.8148229041534707e-05, 'epoch': 1.91}\n",
            " 64% 17500/27471 [49:57<28:22,  5.86it/s]Saving model checkpoint to ./trained_model_3/checkpoint-17500\n",
            "Configuration saved in ./trained_model_3/checkpoint-17500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-17500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-17500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-17500/special_tokens_map.json\n",
            "{'loss': 0.3421, 'learning_rate': 1.7238178442721418e-05, 'epoch': 1.97}\n",
            " 66% 18000/27471 [51:23<26:49,  5.89it/s]Saving model checkpoint to ./trained_model_3/checkpoint-18000\n",
            "Configuration saved in ./trained_model_3/checkpoint-18000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-18000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-18000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-18000/special_tokens_map.json\n",
            "{'loss': 0.3196, 'learning_rate': 1.632812784390812e-05, 'epoch': 2.02}\n",
            " 67% 18500/27471 [52:48<25:21,  5.90it/s]Saving model checkpoint to ./trained_model_3/checkpoint-18500\n",
            "Configuration saved in ./trained_model_3/checkpoint-18500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-18500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-18500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-18500/special_tokens_map.json\n",
            "{'loss': 0.3025, 'learning_rate': 1.541807724509483e-05, 'epoch': 2.07}\n",
            " 69% 19000/27471 [54:14<23:59,  5.89it/s]Saving model checkpoint to ./trained_model_3/checkpoint-19000\n",
            "Configuration saved in ./trained_model_3/checkpoint-19000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-19000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-19000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-19000/special_tokens_map.json\n",
            "{'loss': 0.2945, 'learning_rate': 1.4508026646281533e-05, 'epoch': 2.13}\n",
            " 71% 19500/27471 [55:39<22:32,  5.89it/s]Saving model checkpoint to ./trained_model_3/checkpoint-19500\n",
            "Configuration saved in ./trained_model_3/checkpoint-19500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-19500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-19500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-19500/special_tokens_map.json\n",
            "{'loss': 0.2983, 'learning_rate': 1.3597976047468238e-05, 'epoch': 2.18}\n",
            " 73% 20000/27471 [57:05<21:05,  5.90it/s]Saving model checkpoint to ./trained_model_3/checkpoint-20000\n",
            "Configuration saved in ./trained_model_3/checkpoint-20000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-20000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-20000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-20000/special_tokens_map.json\n",
            "{'loss': 0.2951, 'learning_rate': 1.2687925448654947e-05, 'epoch': 2.24}\n",
            " 75% 20500/27471 [58:30<19:46,  5.87it/s]Saving model checkpoint to ./trained_model_3/checkpoint-20500\n",
            "Configuration saved in ./trained_model_3/checkpoint-20500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-20500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-20500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-20500/special_tokens_map.json\n",
            "{'loss': 0.295, 'learning_rate': 1.1777874849841651e-05, 'epoch': 2.29}\n",
            " 76% 21000/27471 [59:56<18:19,  5.89it/s]Saving model checkpoint to ./trained_model_3/checkpoint-21000\n",
            "Configuration saved in ./trained_model_3/checkpoint-21000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-21000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-21000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-21000/special_tokens_map.json\n",
            "{'loss': 0.2984, 'learning_rate': 1.0867824251028358e-05, 'epoch': 2.35}\n",
            " 78% 21500/27471 [1:01:21<17:08,  5.81it/s]Saving model checkpoint to ./trained_model_3/checkpoint-21500\n",
            "Configuration saved in ./trained_model_3/checkpoint-21500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-21500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-21500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-21500/special_tokens_map.json\n",
            "{'loss': 0.2982, 'learning_rate': 9.957773652215064e-06, 'epoch': 2.4}\n",
            " 80% 22000/27471 [1:02:47<15:29,  5.88it/s]Saving model checkpoint to ./trained_model_3/checkpoint-22000\n",
            "Configuration saved in ./trained_model_3/checkpoint-22000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-22000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-22000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-22000/special_tokens_map.json\n",
            "{'loss': 0.2937, 'learning_rate': 9.047723053401771e-06, 'epoch': 2.46}\n",
            " 82% 22500/27471 [1:04:13<14:00,  5.92it/s]Saving model checkpoint to ./trained_model_3/checkpoint-22500\n",
            "Configuration saved in ./trained_model_3/checkpoint-22500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-22500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-22500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-22500/special_tokens_map.json\n",
            "{'loss': 0.2979, 'learning_rate': 8.137672454588475e-06, 'epoch': 2.51}\n",
            " 84% 23000/27471 [1:05:38<12:36,  5.91it/s]Saving model checkpoint to ./trained_model_3/checkpoint-23000\n",
            "Configuration saved in ./trained_model_3/checkpoint-23000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-23000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-23000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-23000/special_tokens_map.json\n",
            "{'loss': 0.298, 'learning_rate': 7.227621855775181e-06, 'epoch': 2.57}\n",
            " 86% 23500/27471 [1:07:03<11:11,  5.91it/s]Saving model checkpoint to ./trained_model_3/checkpoint-23500\n",
            "Configuration saved in ./trained_model_3/checkpoint-23500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-23500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-23500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-23500/special_tokens_map.json\n",
            "{'loss': 0.2962, 'learning_rate': 6.3175712569618874e-06, 'epoch': 2.62}\n",
            " 87% 24000/27471 [1:08:29<09:48,  5.90it/s]Saving model checkpoint to ./trained_model_3/checkpoint-24000\n",
            "Configuration saved in ./trained_model_3/checkpoint-24000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-24000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-24000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-24000/special_tokens_map.json\n",
            "{'loss': 0.2974, 'learning_rate': 5.407520658148593e-06, 'epoch': 2.68}\n",
            " 89% 24500/27471 [1:09:54<08:21,  5.93it/s]Saving model checkpoint to ./trained_model_3/checkpoint-24500\n",
            "Configuration saved in ./trained_model_3/checkpoint-24500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-24500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-24500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-24500/special_tokens_map.json\n",
            "{'loss': 0.2909, 'learning_rate': 4.497470059335299e-06, 'epoch': 2.73}\n",
            " 91% 25000/27471 [1:11:19<07:02,  5.85it/s]Saving model checkpoint to ./trained_model_3/checkpoint-25000\n",
            "Configuration saved in ./trained_model_3/checkpoint-25000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-25000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-25000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-25000/special_tokens_map.json\n",
            "{'loss': 0.3029, 'learning_rate': 3.5874194605220053e-06, 'epoch': 2.78}\n",
            " 93% 25500/27471 [1:12:44<05:34,  5.90it/s]Saving model checkpoint to ./trained_model_3/checkpoint-25500\n",
            "Configuration saved in ./trained_model_3/checkpoint-25500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-25500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-25500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-25500/special_tokens_map.json\n",
            "{'loss': 0.2936, 'learning_rate': 2.677368861708711e-06, 'epoch': 2.84}\n",
            " 95% 26000/27471 [1:14:10<04:09,  5.88it/s]Saving model checkpoint to ./trained_model_3/checkpoint-26000\n",
            "Configuration saved in ./trained_model_3/checkpoint-26000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-26000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-26000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-26000/special_tokens_map.json\n",
            "{'loss': 0.2936, 'learning_rate': 1.767318262895417e-06, 'epoch': 2.89}\n",
            " 96% 26500/27471 [1:15:35<02:46,  5.85it/s]Saving model checkpoint to ./trained_model_3/checkpoint-26500\n",
            "Configuration saved in ./trained_model_3/checkpoint-26500/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-26500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-26500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-26500/special_tokens_map.json\n",
            "{'loss': 0.292, 'learning_rate': 8.57267664082123e-07, 'epoch': 2.95}\n",
            " 98% 27000/27471 [1:17:01<01:20,  5.86it/s]Saving model checkpoint to ./trained_model_3/checkpoint-27000\n",
            "Configuration saved in ./trained_model_3/checkpoint-27000/config.json\n",
            "Model weights saved in ./trained_model_3/checkpoint-27000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/checkpoint-27000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/checkpoint-27000/special_tokens_map.json\n",
            "100% 27470/27471 [1:18:21<00:00,  5.88it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 4701.3544, 'train_samples_per_second': 350.559, 'train_steps_per_second': 5.843, 'train_loss': 0.36371046849955835, 'epoch': 3.0}\n",
            "100% 27471/27471 [1:18:21<00:00,  5.84it/s]\n",
            "Saving model checkpoint to ./trained_model_3/\n",
            "Configuration saved in ./trained_model_3/config.json\n",
            "Model weights saved in ./trained_model_3/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_3/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_3/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4zowruaRpBi",
        "outputId": "bbc02b69-4cde-4ee9-f4c9-506f44c25a49"
      },
      "source": [
        "#eval on anli[dev_r1]\n",
        "!python3 run-conc.py --do_eval --task nli --dataset anli --concatenate none --eval_split dev_r1 --model ./trained_model_1/ --output_dir ./eval_output_1a/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reusing dataset anli (/root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b)\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-dee798ba540283a0.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-58ba46ef8e5b94d9.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-463cea97051aab54.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-ab033209f9ad42cb.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-1233a8a1c20a12a3.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-8524e79e437f66fd.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-2bfb9f21138d07c7.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-eeb5c0df588c2d46.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-eba49f9ca4f71617.arrow\n",
            " #0:   0% 0/1 [00:00<?, ?ba/s]\n",
            " #1:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\n",
            " #1: 100% 1/1 [00:00<00:00,  3.88ba/s]\n",
            " #0: 100% 1/1 [00:00<00:00,  3.74ba/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "100% 125/125 [00:02<00:00, 58.62it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 1.934457778930664, 'eval_accuracy': 0.3190000057220459, 'eval_runtime': 2.1731, 'eval_samples_per_second': 460.176, 'eval_steps_per_second': 57.522}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT7nX6a4R9aI",
        "outputId": "f0971d61-4bd0-4bd1-8760-00bc776c0a06"
      },
      "source": [
        "#eval on anli[dev_r3]\n",
        "!python3 run-conc.py --do_eval --task nli --dataset anli --concatenate none --eval_split dev_r3 --model ./trained_model_1/ --output_dir ./eval_output_1b/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reusing dataset anli (/root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b)\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-dee798ba540283a0.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-58ba46ef8e5b94d9.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-463cea97051aab54.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-ab033209f9ad42cb.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-1233a8a1c20a12a3.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-8524e79e437f66fd.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-2bfb9f21138d07c7.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-eeb5c0df588c2d46.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-eba49f9ca4f71617.arrow\n",
            " #0:   0% 0/1 [00:00<?, ?ba/s]\n",
            " #1:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\n",
            " #1: 100% 1/1 [00:00<00:00,  3.67ba/s]\n",
            " #0: 100% 1/1 [00:00<00:00,  3.55ba/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1200\n",
            "  Batch size = 8\n",
            "100% 150/150 [00:02<00:00, 62.03it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 1.7683874368667603, 'eval_accuracy': 0.3308333456516266, 'eval_runtime': 2.4418, 'eval_samples_per_second': 491.439, 'eval_steps_per_second': 61.43}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R27_P5qeIif2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeapTzdWR9vT",
        "outputId": "a0e04b50-780c-49be-c786-5ad8f80aeddd"
      },
      "source": [
        "#eval on snli\n",
        "!python3 run-conc.py --do_eval --task nli --dataset snli --concatenate none --model ./trained_model_1/ --output_dir ./eval_output_1c/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reusing dataset snli (/root/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-be8c311388e5d6eb.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-39e7bac6fcfd8931.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-d228f431068e685f.arrow\n",
            " #0:   0% 0/5 [00:00<?, ?ba/s]\n",
            " #0:  20% 1/5 [00:00<00:00,  4.50ba/s]\n",
            " #0:  40% 2/5 [00:00<00:00,  5.17ba/s]\n",
            " #0:  60% 3/5 [00:00<00:00,  5.16ba/s]\n",
            " #0:  80% 4/5 [00:00<00:00,  4.89ba/s]\n",
            " #0: 100% 5/5 [00:00<00:00,  5.04ba/s]\n",
            "\n",
            " #1: 100% 5/5 [00:01<00:00,  4.87ba/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 9842\n",
            "  Batch size = 8\n",
            "100% 1231/1231 [00:21<00:00, 58.16it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 0.2927042543888092, 'eval_accuracy': 0.8977850079536438, 'eval_runtime': 21.1879, 'eval_samples_per_second': 464.51, 'eval_steps_per_second': 58.099}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTLQPl5Tjk_3",
        "outputId": "6d94abd9-6206-4755-ec91-0ce23414ea87"
      },
      "source": [
        "#eval on pelz_data\n",
        "!python3 run-conc.py --do_eval --task nli --dataset pelz_data --concatenate none --model ./trained_model_3/ --output_dir ./eval_output_3e/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using custom data configuration default-d7a45b4dfec1469a\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-d7a45b4dfec1469a/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-d7a45b4dfec1469a/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            " #0:   0% 0/1 [00:00<?, ?ba/s]\n",
            " #0: 100% 1/1 [00:00<00:00, 90.24ba/s]\n",
            " #1: 100% 1/1 [00:00<00:00, 102.79ba/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 30\n",
            "  Batch size = 8\n",
            "100% 4/4 [00:00<00:00, 113.23it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 0.8847774863243103, 'eval_accuracy': 0.6666666865348816, 'eval_runtime': 0.0544, 'eval_samples_per_second': 551.253, 'eval_steps_per_second': 73.5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiIDyPOS83Xr",
        "outputId": "9159b3d7-5b68-4a16-83f9-3538819c0bc7"
      },
      "source": [
        "#trained on snli + anli[train_r3]\n",
        "!python3 run-conc.py --do_train --task nli --dataset snli --concatenate train_r3 --per_device_train_batch_size 60 --output_dir ./trained_model_5/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 3.82kB [00:00, 4.73MB/s]       \n",
            "Downloading: 1.90kB [00:00, 2.43MB/s]     \n",
            "Downloading and preparing dataset snli/plain_text (download: 90.17 MiB, generated: 65.51 MiB, post-processed: Unknown size, total: 155.68 MiB) to /root/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b...\n",
            "Downloading: 100% 1.93k/1.93k [00:00<00:00, 2.10MB/s]\n",
            "Downloading: 100% 1.26M/1.26M [00:00<00:00, 47.3MB/s]\n",
            "Downloading: 100% 65.9M/65.9M [00:01<00:00, 43.0MB/s]\n",
            "Downloading: 100% 1.26M/1.26M [00:00<00:00, 52.5MB/s]\n",
            "Dataset snli downloaded and prepared to /root/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b. Subsequent calls will reuse this data.\n",
            "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "100% 10/10 [00:00<00:00, 38.19ba/s]\n",
            "100% 551/551 [00:13<00:00, 39.54ba/s]\n",
            "100% 10/10 [00:00<00:00, 38.33ba/s]\n",
            "Reusing dataset anli (/root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b)\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-dee798ba540283a0.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-58ba46ef8e5b94d9.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-463cea97051aab54.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-ab033209f9ad42cb.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-1233a8a1c20a12a3.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-8524e79e437f66fd.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-2bfb9f21138d07c7.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-eeb5c0df588c2d46.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-eba49f9ca4f71617.arrow\n",
            " #0:   0% 0/325 [00:00<?, ?ba/s]\n",
            " #0:   0% 1/325 [00:00<01:01,  5.30ba/s]\n",
            " #0:   1% 2/325 [00:00<00:57,  5.64ba/s]\n",
            " #0:   1% 3/325 [00:00<00:55,  5.84ba/s]\n",
            " #0:   1% 4/325 [00:00<01:27,  3.66ba/s]\n",
            " #0:   2% 5/325 [00:01<01:14,  4.29ba/s]\n",
            " #0:   2% 6/325 [00:01<01:07,  4.72ba/s]\n",
            " #0:   2% 7/325 [00:01<01:02,  5.07ba/s]\n",
            " #0:   2% 8/325 [00:01<00:59,  5.33ba/s]\n",
            " #0:   3% 9/325 [00:01<00:58,  5.40ba/s]\n",
            " #0:   3% 10/325 [00:01<00:56,  5.56ba/s]\n",
            " #0:   3% 11/325 [00:02<00:56,  5.58ba/s]\n",
            " #0:   4% 12/325 [00:02<00:55,  5.68ba/s]\n",
            " #0:   4% 13/325 [00:02<00:54,  5.75ba/s]\n",
            " #0:   4% 14/325 [00:02<00:54,  5.71ba/s]\n",
            " #0:   5% 15/325 [00:02<00:53,  5.74ba/s]\n",
            " #0:   5% 16/325 [00:03<00:53,  5.78ba/s]\n",
            " #0:   5% 17/325 [00:03<00:53,  5.78ba/s]\n",
            " #0:   6% 18/325 [00:03<00:53,  5.79ba/s]\n",
            " #0:   6% 19/325 [00:03<00:52,  5.83ba/s]\n",
            " #0:   6% 20/325 [00:03<00:52,  5.85ba/s]\n",
            " #0:   6% 21/325 [00:03<00:51,  5.89ba/s]\n",
            " #0:   7% 22/325 [00:04<00:51,  5.91ba/s]\n",
            " #0:   7% 23/325 [00:04<00:51,  5.85ba/s]\n",
            " #0:   7% 24/325 [00:04<00:50,  5.97ba/s]\n",
            " #1:   7% 24/325 [00:04<00:50,  5.99ba/s]\u001b[A\n",
            " #0:   8% 25/325 [00:04<01:03,  4.69ba/s]\n",
            " #0:   8% 26/325 [00:04<01:02,  4.75ba/s]\n",
            " #0:   8% 27/325 [00:05<00:58,  5.10ba/s]\n",
            " #0:   9% 28/325 [00:05<00:56,  5.28ba/s]\n",
            " #0:   9% 29/325 [00:05<00:53,  5.50ba/s]\n",
            " #0:   9% 30/325 [00:05<00:51,  5.71ba/s]\n",
            " #0:  10% 31/325 [00:05<00:50,  5.78ba/s]\n",
            " #0:  10% 32/325 [00:05<00:49,  5.87ba/s]\n",
            " #0:  10% 33/325 [00:06<00:49,  5.92ba/s]\n",
            " #0:  10% 34/325 [00:06<00:49,  5.92ba/s]\n",
            " #0:  11% 35/325 [00:06<00:48,  5.98ba/s]\n",
            " #0:  11% 36/325 [00:06<00:47,  6.05ba/s]\n",
            " #0:  11% 37/325 [00:06<00:48,  5.95ba/s]\n",
            " #0:  12% 38/325 [00:06<00:48,  5.90ba/s]\n",
            " #0:  12% 39/325 [00:07<00:48,  5.89ba/s]\n",
            " #0:  12% 40/325 [00:07<00:48,  5.87ba/s]\n",
            " #0:  13% 41/325 [00:07<00:48,  5.84ba/s]\n",
            " #0:  13% 42/325 [00:07<00:47,  5.92ba/s]\n",
            " #0:  13% 43/325 [00:07<00:47,  5.93ba/s]\n",
            " #0:  14% 44/325 [00:07<00:46,  6.03ba/s]\n",
            " #0:  14% 45/325 [00:08<00:46,  6.01ba/s]\n",
            " #0:  14% 46/325 [00:08<00:59,  4.69ba/s]\n",
            " #0:  14% 47/325 [00:08<00:54,  5.06ba/s]\n",
            " #0:  15% 48/325 [00:08<00:51,  5.35ba/s]\n",
            " #0:  15% 49/325 [00:08<00:49,  5.53ba/s]\n",
            " #0:  15% 50/325 [00:09<00:48,  5.65ba/s]\n",
            " #0:  16% 51/325 [00:09<00:48,  5.69ba/s]\n",
            " #0:  16% 52/325 [00:09<00:47,  5.79ba/s]\n",
            " #0:  16% 53/325 [00:09<00:46,  5.91ba/s]\n",
            " #0:  17% 54/325 [00:09<00:46,  5.89ba/s]\n",
            " #0:  17% 55/325 [00:09<00:45,  5.87ba/s]\n",
            " #0:  17% 56/325 [00:10<00:45,  5.86ba/s]\n",
            " #0:  18% 57/325 [00:10<00:46,  5.77ba/s]\n",
            " #0:  18% 58/325 [00:10<00:49,  5.38ba/s]\n",
            " #0:  18% 59/325 [00:10<00:47,  5.57ba/s]\n",
            " #0:  18% 60/325 [00:10<00:45,  5.77ba/s]\n",
            " #0:  19% 61/325 [00:10<00:45,  5.84ba/s]\n",
            " #0:  19% 62/325 [00:11<00:44,  5.92ba/s]\n",
            " #0:  19% 63/325 [00:11<00:43,  5.96ba/s]\n",
            " #0:  20% 64/325 [00:11<00:43,  6.05ba/s]\n",
            " #0:  20% 65/325 [00:11<00:42,  6.13ba/s]\n",
            " #0:  20% 66/325 [00:11<00:42,  6.11ba/s]\n",
            " #0:  21% 67/325 [00:12<00:55,  4.65ba/s]\n",
            " #0:  21% 68/325 [00:12<00:52,  4.92ba/s]\n",
            " #0:  21% 69/325 [00:12<00:49,  5.20ba/s]\n",
            " #0:  22% 70/325 [00:12<00:46,  5.44ba/s]\n",
            " #0:  22% 71/325 [00:12<00:44,  5.67ba/s]\n",
            " #0:  22% 72/325 [00:12<00:43,  5.84ba/s]\n",
            " #0:  22% 73/325 [00:13<00:42,  5.93ba/s]\n",
            " #0:  23% 74/325 [00:13<00:41,  6.08ba/s]\n",
            " #0:  23% 75/325 [00:13<00:41,  6.02ba/s]\n",
            " #0:  23% 76/325 [00:13<00:40,  6.11ba/s]\n",
            " #0:  24% 77/325 [00:13<00:40,  6.10ba/s]\n",
            " #0:  24% 79/325 [00:14<00:40,  6.04ba/s]\n",
            " #0:  25% 80/325 [00:14<00:40,  6.06ba/s]\n",
            " #1:  25% 80/325 [00:14<00:41,  5.89ba/s]\u001b[A\n",
            " #0:  25% 82/325 [00:14<00:42,  5.77ba/s]\n",
            " #0:  26% 83/325 [00:14<00:41,  5.79ba/s]\n",
            " #0:  26% 84/325 [00:14<00:40,  5.93ba/s]\n",
            " #1:  26% 84/325 [00:14<00:40,  5.91ba/s]\u001b[A\n",
            " #0:  26% 85/325 [00:15<00:40,  5.89ba/s]\n",
            " #0:  26% 86/325 [00:15<00:40,  5.96ba/s]\n",
            " #0:  27% 88/325 [00:15<00:52,  4.54ba/s]\n",
            " #0:  27% 89/325 [00:15<00:49,  4.76ba/s]\n",
            " #0:  28% 90/325 [00:16<00:46,  5.00ba/s]\n",
            " #0:  28% 91/325 [00:16<00:43,  5.36ba/s]\n",
            " #0:  28% 92/325 [00:16<00:42,  5.45ba/s]\n",
            " #0:  29% 93/325 [00:16<00:41,  5.63ba/s]\n",
            " #0:  29% 94/325 [00:16<00:40,  5.71ba/s]\n",
            " #0:  29% 95/325 [00:16<00:40,  5.73ba/s]\n",
            " #0:  30% 96/325 [00:17<00:39,  5.77ba/s]\n",
            " #0:  30% 97/325 [00:17<00:38,  5.90ba/s]\n",
            " #0:  30% 98/325 [00:17<00:38,  5.87ba/s]\n",
            " #0:  30% 99/325 [00:17<00:37,  6.00ba/s]\n",
            " #0:  31% 100/325 [00:17<00:37,  5.98ba/s]\n",
            " #0:  31% 101/325 [00:17<00:37,  6.03ba/s]\n",
            " #0:  31% 102/325 [00:18<00:36,  6.05ba/s]\n",
            " #0:  32% 103/325 [00:18<00:36,  6.14ba/s]\n",
            " #0:  32% 104/325 [00:18<00:36,  6.02ba/s]\n",
            " #0:  32% 105/325 [00:18<00:36,  5.98ba/s]\n",
            " #0:  33% 106/325 [00:18<00:36,  6.00ba/s]\n",
            " #0:  33% 107/325 [00:18<00:36,  6.02ba/s]\n",
            " #0:  33% 108/325 [00:19<00:35,  6.04ba/s]\n",
            " #0:  34% 109/325 [00:19<00:45,  4.70ba/s]\n",
            " #0:  34% 110/325 [00:19<00:42,  5.01ba/s]\n",
            " #0:  34% 111/325 [00:19<00:40,  5.24ba/s]\n",
            " #0:  34% 112/325 [00:19<00:38,  5.50ba/s]\n",
            " #0:  35% 113/325 [00:20<00:37,  5.63ba/s]\n",
            " #0:  35% 114/325 [00:20<00:36,  5.73ba/s]\n",
            " #0:  35% 115/325 [00:20<00:36,  5.74ba/s]\n",
            " #0:  36% 116/325 [00:20<00:35,  5.86ba/s]\n",
            " #0:  36% 117/325 [00:20<00:35,  5.89ba/s]\n",
            " #0:  36% 118/325 [00:20<00:35,  5.87ba/s]\n",
            " #1:  36% 118/325 [00:20<00:34,  5.97ba/s]\u001b[A\n",
            " #0:  37% 119/325 [00:21<00:37,  5.46ba/s]\n",
            " #0:  37% 120/325 [00:21<00:37,  5.51ba/s]\n",
            " #0:  37% 121/325 [00:21<00:36,  5.55ba/s]\n",
            " #0:  38% 122/325 [00:21<00:36,  5.62ba/s]\n",
            " #0:  38% 123/325 [00:21<00:35,  5.65ba/s]\n",
            " #0:  38% 124/325 [00:22<00:35,  5.70ba/s]\n",
            " #0:  38% 125/325 [00:22<00:34,  5.76ba/s]\n",
            " #0:  39% 126/325 [00:22<00:33,  5.91ba/s]\n",
            " #0:  39% 127/325 [00:22<00:33,  5.90ba/s]\n",
            " #0:  39% 128/325 [00:22<00:32,  5.97ba/s]\n",
            " #0:  40% 129/325 [00:22<00:32,  5.97ba/s]\n",
            " #0:  40% 130/325 [00:23<00:42,  4.64ba/s]\n",
            " #0:  40% 131/325 [00:23<00:38,  4.99ba/s]\n",
            " #0:  41% 132/325 [00:23<00:37,  5.19ba/s]\n",
            " #0:  41% 133/325 [00:23<00:35,  5.45ba/s]\n",
            " #0:  41% 134/325 [00:23<00:34,  5.58ba/s]\n",
            " #0:  42% 135/325 [00:24<00:33,  5.72ba/s]\n",
            " #0:  42% 136/325 [00:24<00:32,  5.85ba/s]\n",
            " #0:  42% 137/325 [00:24<00:31,  5.93ba/s]\n",
            " #0:  42% 138/325 [00:24<00:31,  5.87ba/s]\n",
            " #0:  43% 139/325 [00:24<00:31,  5.92ba/s]\n",
            " #0:  43% 140/325 [00:24<00:30,  5.98ba/s]\n",
            " #0:  43% 141/325 [00:25<00:30,  6.01ba/s]\n",
            " #0:  44% 142/325 [00:25<00:30,  6.10ba/s]\n",
            " #0:  44% 143/325 [00:25<00:29,  6.16ba/s]\n",
            " #0:  45% 145/325 [00:25<00:29,  6.11ba/s]\n",
            " #0:  45% 146/325 [00:25<00:29,  6.05ba/s]\n",
            " #0:  45% 147/325 [00:26<00:30,  5.87ba/s]\n",
            " #0:  46% 148/325 [00:26<00:29,  5.92ba/s]\n",
            " #0:  46% 149/325 [00:26<00:29,  6.02ba/s]\n",
            " #0:  46% 150/325 [00:26<00:28,  6.09ba/s]\n",
            " #0:  46% 151/325 [00:26<00:38,  4.57ba/s]\n",
            " #0:  47% 152/325 [00:27<00:35,  4.92ba/s]\n",
            " #0:  47% 153/325 [00:27<00:32,  5.23ba/s]\n",
            " #0:  47% 154/325 [00:27<00:30,  5.53ba/s]\n",
            " #0:  48% 155/325 [00:27<00:29,  5.68ba/s]\n",
            " #0:  48% 156/325 [00:27<00:29,  5.80ba/s]\n",
            " #0:  48% 157/325 [00:27<00:28,  5.94ba/s]\n",
            " #0:  49% 158/325 [00:27<00:27,  6.01ba/s]\n",
            " #0:  49% 159/325 [00:28<00:27,  6.06ba/s]\n",
            " #0:  49% 160/325 [00:28<00:27,  6.04ba/s]\n",
            " #0:  50% 161/325 [00:28<00:27,  5.94ba/s]\n",
            " #0:  50% 162/325 [00:28<00:28,  5.79ba/s]\n",
            " #0:  50% 163/325 [00:28<00:27,  5.88ba/s]\n",
            " #0:  50% 164/325 [00:28<00:27,  5.91ba/s]\n",
            " #0:  51% 165/325 [00:29<00:26,  6.09ba/s]\n",
            " #0:  51% 166/325 [00:29<00:25,  6.13ba/s]\n",
            " #0:  51% 167/325 [00:29<00:25,  6.16ba/s]\n",
            " #0:  52% 168/325 [00:29<00:26,  5.94ba/s]\n",
            " #0:  52% 169/325 [00:29<00:26,  5.98ba/s]\n",
            " #0:  52% 170/325 [00:29<00:25,  6.02ba/s]\n",
            " #0:  53% 171/325 [00:30<00:25,  6.04ba/s]\n",
            " #0:  53% 172/325 [00:30<00:32,  4.73ba/s]\n",
            " #0:  53% 173/325 [00:30<00:30,  5.06ba/s]\n",
            " #0:  54% 174/325 [00:30<00:28,  5.32ba/s]\n",
            " #0:  54% 175/325 [00:30<00:26,  5.58ba/s]\n",
            " #0:  54% 176/325 [00:31<00:25,  5.77ba/s]\n",
            " #0:  55% 178/325 [00:31<00:25,  5.85ba/s]\n",
            " #0:  55% 179/325 [00:31<00:25,  5.78ba/s]\n",
            " #0:  55% 180/325 [00:31<00:24,  5.87ba/s]\n",
            " #0:  56% 181/325 [00:31<00:24,  5.92ba/s]\n",
            " #0:  56% 182/325 [00:32<00:23,  6.00ba/s]\n",
            " #0:  56% 183/325 [00:32<00:23,  6.03ba/s]\n",
            " #0:  57% 184/325 [00:32<00:23,  6.05ba/s]\n",
            " #0:  57% 185/325 [00:32<00:24,  5.77ba/s]\n",
            " #0:  57% 186/325 [00:32<00:23,  5.87ba/s]\n",
            " #0:  58% 187/325 [00:32<00:23,  5.88ba/s]\n",
            " #0:  58% 188/325 [00:33<00:22,  6.02ba/s]\n",
            " #0:  58% 189/325 [00:33<00:22,  6.06ba/s]\n",
            " #0:  58% 190/325 [00:33<00:22,  6.13ba/s]\n",
            " #0:  59% 191/325 [00:33<00:22,  6.09ba/s]\n",
            " #0:  59% 192/325 [00:33<00:22,  5.95ba/s]\n",
            " #1:  59% 191/325 [00:33<00:22,  5.93ba/s]\u001b[A\n",
            " #0:  60% 194/325 [00:34<00:25,  5.17ba/s]\n",
            " #0:  60% 195/325 [00:34<00:24,  5.38ba/s]\n",
            " #0:  60% 196/325 [00:34<00:23,  5.60ba/s]\n",
            " #0:  61% 197/325 [00:34<00:22,  5.63ba/s]\n",
            " #0:  61% 198/325 [00:34<00:21,  5.85ba/s]\n",
            " #0:  61% 199/325 [00:35<00:21,  5.91ba/s]\n",
            " #0:  62% 200/325 [00:35<00:20,  6.02ba/s]\n",
            " #0:  62% 201/325 [00:35<00:20,  6.08ba/s]\n",
            " #0:  62% 202/325 [00:35<00:20,  6.07ba/s]\n",
            " #0:  62% 203/325 [00:35<00:20,  5.89ba/s]\n",
            " #0:  63% 204/325 [00:35<00:20,  5.96ba/s]\n",
            " #0:  63% 205/325 [00:36<00:20,  5.98ba/s]\n",
            " #0:  63% 206/325 [00:36<00:20,  5.91ba/s]\n",
            " #0:  64% 207/325 [00:36<00:19,  6.06ba/s]\n",
            " #0:  64% 208/325 [00:36<00:21,  5.52ba/s]\n",
            " #0:  64% 209/325 [00:36<00:21,  5.46ba/s]\n",
            " #0:  65% 210/325 [00:36<00:20,  5.49ba/s]\n",
            " #0:  65% 211/325 [00:37<00:20,  5.63ba/s]\n",
            " #0:  65% 212/325 [00:37<00:19,  5.80ba/s]\n",
            " #0:  66% 213/325 [00:37<00:19,  5.85ba/s]\n",
            " #1:  65% 212/325 [00:37<00:19,  5.92ba/s]\u001b[A\n",
            " #0:  66% 215/325 [00:37<00:22,  4.94ba/s]\n",
            " #0:  66% 216/325 [00:38<00:20,  5.25ba/s]\n",
            " #0:  67% 217/325 [00:38<00:20,  5.37ba/s]\n",
            " #0:  67% 218/325 [00:38<00:19,  5.57ba/s]\n",
            " #0:  67% 219/325 [00:38<00:18,  5.70ba/s]\n",
            " #0:  68% 220/325 [00:38<00:18,  5.78ba/s]\n",
            " #0:  68% 221/325 [00:38<00:17,  5.90ba/s]\n",
            " #0:  68% 222/325 [00:39<00:17,  5.98ba/s]\n",
            " #0:  69% 223/325 [00:39<00:16,  6.07ba/s]\n",
            " #0:  69% 224/325 [00:39<00:16,  6.11ba/s]\n",
            " #0:  69% 225/325 [00:39<00:16,  6.19ba/s]\n",
            " #0:  70% 227/325 [00:39<00:16,  6.06ba/s]\n",
            " #0:  71% 230/325 [00:40<00:15,  6.14ba/s]\n",
            " #0:  71% 232/325 [00:40<00:15,  6.06ba/s]\n",
            " #0:  72% 234/325 [00:41<00:14,  6.09ba/s]\n",
            " #0:  73% 236/325 [00:41<00:17,  5.14ba/s]\n",
            " #0:  73% 238/325 [00:41<00:15,  5.58ba/s]\n",
            " #0:  74% 241/325 [00:42<00:14,  5.93ba/s]\n",
            " #0:  75% 243/325 [00:42<00:13,  5.96ba/s]\n",
            " #0:  75% 245/325 [00:43<00:13,  5.84ba/s]\n",
            " #0:  76% 247/325 [00:43<00:12,  6.01ba/s]\n",
            " #0:  77% 251/325 [00:44<00:12,  6.11ba/s]\n",
            " #0:  78% 253/325 [00:44<00:11,  6.14ba/s]\n",
            " #0:  78% 255/325 [00:44<00:11,  6.08ba/s]\n",
            " #0:  79% 257/325 [00:45<00:13,  5.01ba/s]\n",
            " #0:  80% 260/325 [00:45<00:11,  5.73ba/s]\n",
            " #0:  81% 262/325 [00:46<00:10,  5.79ba/s]\n",
            " #0:  81% 264/325 [00:46<00:10,  5.94ba/s]\n",
            " #0:  82% 266/325 [00:46<00:09,  6.04ba/s]\n",
            " #0:  83% 269/325 [00:47<00:09,  5.82ba/s]\n",
            " #0:  83% 271/325 [00:47<00:09,  5.60ba/s]\n",
            " #0:  84% 273/325 [00:47<00:09,  5.66ba/s]\n",
            " #0:  85% 276/325 [00:48<00:08,  5.81ba/s]\n",
            " #0:  85% 277/325 [00:48<00:10,  4.65ba/s]\n",
            " #0:  86% 279/325 [00:49<00:08,  5.19ba/s]\n",
            " #0:  87% 282/325 [00:49<00:07,  5.69ba/s]\n",
            " #0:  87% 284/325 [00:49<00:06,  5.90ba/s]\n",
            " #0:  88% 286/325 [00:50<00:06,  6.05ba/s]\n",
            " #0:  89% 289/325 [00:50<00:05,  6.06ba/s]\n",
            " #0:  90% 291/325 [00:51<00:05,  5.94ba/s]\n",
            " #0:  90% 293/325 [00:51<00:05,  6.04ba/s]\n",
            " #0:  91% 296/325 [00:51<00:04,  5.88ba/s]\n",
            " #0:  92% 298/325 [00:52<00:05,  4.60ba/s]\n",
            " #0:  92% 300/325 [00:52<00:04,  5.11ba/s]\n",
            " #0:  93% 303/325 [00:53<00:03,  5.71ba/s]\n",
            " #0:  94% 305/325 [00:53<00:03,  5.96ba/s]\n",
            " #0:  95% 308/325 [00:54<00:02,  5.94ba/s]\n",
            " #0:  95% 310/325 [00:54<00:02,  6.06ba/s]\n",
            " #0:  96% 312/325 [00:54<00:02,  6.05ba/s]\n",
            " #0:  97% 315/325 [00:55<00:01,  6.00ba/s]\n",
            " #0:  98% 317/325 [00:55<00:01,  6.07ba/s]\n",
            " #0:  98% 318/325 [00:55<00:01,  6.02ba/s]\n",
            " #0:  99% 321/325 [00:56<00:00,  5.25ba/s]\n",
            " #0:  99% 323/325 [00:56<00:00,  5.68ba/s]\n",
            " #0: 100% 325/325 [00:57<00:00,  5.70ba/s]\n",
            "\n",
            " #1:  82% 268/325 [00:57<00:21,  2.66ba/s]\u001b[A\n",
            " #1:  83% 269/325 [00:57<00:18,  3.07ba/s]\u001b[A\n",
            " #1:  83% 270/325 [00:57<00:16,  3.42ba/s]\u001b[A\n",
            " #1:  83% 271/325 [00:57<00:14,  3.68ba/s]\u001b[A\n",
            " #1:  84% 272/325 [00:58<00:13,  3.86ba/s]\u001b[A\n",
            " #1:  84% 273/325 [00:58<00:12,  4.01ba/s]\u001b[A\n",
            " #1:  84% 274/325 [00:58<00:12,  4.18ba/s]\u001b[A\n",
            " #1:  85% 275/325 [00:58<00:11,  4.30ba/s]\u001b[A\n",
            " #1:  85% 276/325 [00:58<00:11,  4.41ba/s]\u001b[A\n",
            " #1:  85% 277/325 [00:59<00:12,  3.89ba/s]\u001b[A\n",
            " #1:  86% 278/325 [00:59<00:11,  4.10ba/s]\u001b[A\n",
            " #1:  86% 279/325 [00:59<00:10,  4.25ba/s]\u001b[A\n",
            " #1:  86% 280/325 [00:59<00:10,  4.32ba/s]\u001b[A\n",
            " #1:  86% 281/325 [01:00<00:10,  4.37ba/s]\u001b[A\n",
            " #1:  87% 282/325 [01:00<00:09,  4.40ba/s]\u001b[A\n",
            " #1:  87% 283/325 [01:00<00:09,  4.51ba/s]\u001b[A\n",
            " #1:  87% 284/325 [01:00<00:09,  4.52ba/s]\u001b[A\n",
            " #1:  88% 285/325 [01:00<00:08,  4.51ba/s]\u001b[A\n",
            " #1:  88% 286/325 [01:01<00:08,  4.47ba/s]\u001b[A\n",
            " #1:  88% 287/325 [01:01<00:08,  4.42ba/s]\u001b[A\n",
            " #1:  89% 288/325 [01:01<00:08,  4.40ba/s]\u001b[A\n",
            " #1:  89% 289/325 [01:01<00:08,  4.32ba/s]\u001b[A\n",
            " #1:  89% 290/325 [01:02<00:08,  4.29ba/s]\u001b[A\n",
            " #1:  90% 291/325 [01:02<00:07,  4.33ba/s]\u001b[A\n",
            " #1:  90% 292/325 [01:02<00:07,  4.35ba/s]\u001b[A\n",
            " #1:  90% 293/325 [01:02<00:07,  4.20ba/s]\u001b[A\n",
            " #1:  90% 294/325 [01:03<00:07,  4.17ba/s]\u001b[A\n",
            " #1:  91% 295/325 [01:03<00:07,  4.19ba/s]\u001b[A\n",
            " #1:  91% 296/325 [01:03<00:06,  4.26ba/s]\u001b[A\n",
            " #1:  91% 297/325 [01:03<00:06,  4.36ba/s]\u001b[A\n",
            " #1:  92% 298/325 [01:04<00:07,  3.82ba/s]\u001b[A\n",
            " #1:  92% 299/325 [01:04<00:06,  3.97ba/s]\u001b[A\n",
            " #1:  92% 300/325 [01:04<00:06,  4.10ba/s]\u001b[A\n",
            " #1:  93% 301/325 [01:04<00:05,  4.21ba/s]\u001b[A\n",
            " #1:  93% 302/325 [01:05<00:05,  4.35ba/s]\u001b[A\n",
            " #1:  93% 303/325 [01:05<00:05,  4.35ba/s]\u001b[A\n",
            " #1:  94% 304/325 [01:05<00:04,  4.40ba/s]\u001b[A\n",
            " #1:  94% 305/325 [01:05<00:04,  4.46ba/s]\u001b[A\n",
            " #1:  94% 306/325 [01:05<00:04,  4.53ba/s]\u001b[A\n",
            " #1:  94% 307/325 [01:06<00:03,  4.59ba/s]\u001b[A\n",
            " #1:  95% 308/325 [01:06<00:03,  4.57ba/s]\u001b[A\n",
            " #1:  95% 309/325 [01:06<00:03,  4.59ba/s]\u001b[A\n",
            " #1:  95% 310/325 [01:06<00:03,  4.57ba/s]\u001b[A\n",
            " #1:  96% 311/325 [01:06<00:03,  4.59ba/s]\u001b[A\n",
            " #1:  96% 312/325 [01:07<00:02,  4.49ba/s]\u001b[A\n",
            " #1:  96% 313/325 [01:07<00:02,  4.58ba/s]\u001b[A\n",
            " #1:  97% 314/325 [01:07<00:02,  4.58ba/s]\u001b[A\n",
            " #1:  97% 315/325 [01:07<00:02,  4.58ba/s]\u001b[A\n",
            " #1:  97% 316/325 [01:08<00:01,  4.55ba/s]\u001b[A\n",
            " #1:  98% 317/325 [01:08<00:01,  4.47ba/s]\u001b[A\n",
            " #1:  98% 318/325 [01:08<00:01,  4.68ba/s]\u001b[A\n",
            " #1:  98% 319/325 [01:08<00:01,  4.11ba/s]\u001b[A\n",
            " #1:  98% 320/325 [01:08<00:01,  4.41ba/s]\u001b[A\n",
            " #1:  99% 321/325 [01:09<00:00,  4.65ba/s]\u001b[A\n",
            " #1:  99% 322/325 [01:09<00:00,  4.80ba/s]\u001b[A\n",
            " #1:  99% 323/325 [01:09<00:00,  4.98ba/s]\u001b[A\n",
            " #1: 100% 324/325 [01:09<00:00,  5.12ba/s]\u001b[A\n",
            " #1: 100% 325/325 [01:09<00:00,  4.65ba/s]\n",
            "***** Running training *****\n",
            "  Num examples = 649826\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 60\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 60\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 32493\n",
            "{'loss': 0.7809, 'learning_rate': 4.923060351460315e-05, 'epoch': 0.05}\n",
            "  2% 500/32493 [01:24<1:30:07,  5.92it/s]Saving model checkpoint to ./trained_model_5/checkpoint-500\n",
            "Configuration saved in ./trained_model_5/checkpoint-500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.6081, 'learning_rate': 4.846120702920629e-05, 'epoch': 0.09}\n",
            "  3% 1000/32493 [02:49<1:28:51,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-1000\n",
            "Configuration saved in ./trained_model_5/checkpoint-1000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.5646, 'learning_rate': 4.769181054380944e-05, 'epoch': 0.14}\n",
            "  5% 1500/32493 [04:14<1:27:26,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-1500\n",
            "Configuration saved in ./trained_model_5/checkpoint-1500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 0.5397, 'learning_rate': 4.6922414058412586e-05, 'epoch': 0.18}\n",
            "  6% 2000/32493 [05:39<1:25:43,  5.93it/s]Saving model checkpoint to ./trained_model_5/checkpoint-2000\n",
            "Configuration saved in ./trained_model_5/checkpoint-2000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 0.5216, 'learning_rate': 4.6153017573015725e-05, 'epoch': 0.23}\n",
            "  8% 2500/32493 [07:04<1:24:30,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-2500\n",
            "Configuration saved in ./trained_model_5/checkpoint-2500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 0.5147, 'learning_rate': 4.538362108761887e-05, 'epoch': 0.28}\n",
            "  9% 3000/32493 [08:29<1:23:24,  5.89it/s]Saving model checkpoint to ./trained_model_5/checkpoint-3000\n",
            "Configuration saved in ./trained_model_5/checkpoint-3000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 0.493, 'learning_rate': 4.4614224602222024e-05, 'epoch': 0.32}\n",
            " 11% 3500/32493 [09:54<1:21:44,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-3500\n",
            "Configuration saved in ./trained_model_5/checkpoint-3500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 0.4879, 'learning_rate': 4.384482811682516e-05, 'epoch': 0.37}\n",
            " 12% 4000/32493 [11:20<1:19:51,  5.95it/s]Saving model checkpoint to ./trained_model_5/checkpoint-4000\n",
            "Configuration saved in ./trained_model_5/checkpoint-4000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 0.4832, 'learning_rate': 4.307543163142831e-05, 'epoch': 0.42}\n",
            " 14% 4500/32493 [12:45<1:19:06,  5.90it/s]Saving model checkpoint to ./trained_model_5/checkpoint-4500\n",
            "Configuration saved in ./trained_model_5/checkpoint-4500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-4500/special_tokens_map.json\n",
            "{'loss': 0.473, 'learning_rate': 4.2306035146031455e-05, 'epoch': 0.46}\n",
            " 15% 5000/32493 [14:10<1:17:23,  5.92it/s]Saving model checkpoint to ./trained_model_5/checkpoint-5000\n",
            "Configuration saved in ./trained_model_5/checkpoint-5000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-5000/special_tokens_map.json\n",
            "{'loss': 0.4681, 'learning_rate': 4.15366386606346e-05, 'epoch': 0.51}\n",
            " 17% 5500/32493 [15:35<1:16:04,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-5500\n",
            "Configuration saved in ./trained_model_5/checkpoint-5500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-5500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-5500/special_tokens_map.json\n",
            "{'loss': 0.4575, 'learning_rate': 4.076724217523775e-05, 'epoch': 0.55}\n",
            " 18% 6000/32493 [17:00<1:14:45,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-6000\n",
            "Configuration saved in ./trained_model_5/checkpoint-6000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-6000/special_tokens_map.json\n",
            "{'loss': 0.4578, 'learning_rate': 3.999784568984089e-05, 'epoch': 0.6}\n",
            " 20% 6500/32493 [18:25<1:13:18,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-6500\n",
            "Configuration saved in ./trained_model_5/checkpoint-6500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-6500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-6500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-6500/special_tokens_map.json\n",
            "{'loss': 0.4448, 'learning_rate': 3.922844920444403e-05, 'epoch': 0.65}\n",
            " 22% 7000/32493 [19:51<1:11:45,  5.92it/s]Saving model checkpoint to ./trained_model_5/checkpoint-7000\n",
            "Configuration saved in ./trained_model_5/checkpoint-7000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-7000/special_tokens_map.json\n",
            "{'loss': 0.4523, 'learning_rate': 3.8459052719047185e-05, 'epoch': 0.69}\n",
            " 23% 7500/32493 [21:16<1:10:21,  5.92it/s]Saving model checkpoint to ./trained_model_5/checkpoint-7500\n",
            "Configuration saved in ./trained_model_5/checkpoint-7500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-7500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-7500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-7500/special_tokens_map.json\n",
            "{'loss': 0.4512, 'learning_rate': 3.768965623365033e-05, 'epoch': 0.74}\n",
            " 25% 8000/32493 [22:41<1:09:32,  5.87it/s]Saving model checkpoint to ./trained_model_5/checkpoint-8000\n",
            "Configuration saved in ./trained_model_5/checkpoint-8000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-8000/special_tokens_map.json\n",
            "{'loss': 0.4398, 'learning_rate': 3.692025974825347e-05, 'epoch': 0.78}\n",
            " 26% 8500/32493 [24:06<1:07:42,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-8500\n",
            "Configuration saved in ./trained_model_5/checkpoint-8500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-8500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-8500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-8500/special_tokens_map.json\n",
            "{'loss': 0.4368, 'learning_rate': 3.6150863262856616e-05, 'epoch': 0.83}\n",
            " 28% 9000/32493 [25:32<1:06:12,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-9000\n",
            "Configuration saved in ./trained_model_5/checkpoint-9000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-9000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-9000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-9000/special_tokens_map.json\n",
            "{'loss': 0.4296, 'learning_rate': 3.538146677745976e-05, 'epoch': 0.88}\n",
            " 29% 9500/32493 [26:57<1:04:45,  5.92it/s]Saving model checkpoint to ./trained_model_5/checkpoint-9500\n",
            "Configuration saved in ./trained_model_5/checkpoint-9500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-9500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-9500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-9500/special_tokens_map.json\n",
            "{'loss': 0.4273, 'learning_rate': 3.461207029206291e-05, 'epoch': 0.92}\n",
            " 31% 10000/32493 [28:22<1:03:29,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-10000\n",
            "Configuration saved in ./trained_model_5/checkpoint-10000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-10000/special_tokens_map.json\n",
            "{'loss': 0.4222, 'learning_rate': 3.3842673806666054e-05, 'epoch': 0.97}\n",
            " 32% 10500/32493 [29:47<1:02:43,  5.84it/s]Saving model checkpoint to ./trained_model_5/checkpoint-10500\n",
            "Configuration saved in ./trained_model_5/checkpoint-10500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-10500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-10500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-10500/special_tokens_map.json\n",
            "{'loss': 0.4103, 'learning_rate': 3.30732773212692e-05, 'epoch': 1.02}\n",
            " 34% 11000/32493 [31:12<1:00:52,  5.88it/s]Saving model checkpoint to ./trained_model_5/checkpoint-11000\n",
            "Configuration saved in ./trained_model_5/checkpoint-11000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-11000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-11000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-11000/special_tokens_map.json\n",
            "{'loss': 0.3832, 'learning_rate': 3.230388083587234e-05, 'epoch': 1.06}\n",
            " 35% 11500/32493 [32:37<59:14,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-11500\n",
            "Configuration saved in ./trained_model_5/checkpoint-11500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-11500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-11500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-11500/special_tokens_map.json\n",
            "{'loss': 0.3867, 'learning_rate': 3.153448435047549e-05, 'epoch': 1.11}\n",
            " 37% 12000/32493 [34:03<57:36,  5.93it/s]Saving model checkpoint to ./trained_model_5/checkpoint-12000\n",
            "Configuration saved in ./trained_model_5/checkpoint-12000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-12000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-12000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-12000/special_tokens_map.json\n",
            "{'loss': 0.3746, 'learning_rate': 3.076508786507864e-05, 'epoch': 1.15}\n",
            " 38% 12500/32493 [35:28<56:22,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-12500\n",
            "Configuration saved in ./trained_model_5/checkpoint-12500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-12500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-12500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-12500/special_tokens_map.json\n",
            "{'loss': 0.3803, 'learning_rate': 2.9995691379681777e-05, 'epoch': 1.2}\n",
            " 40% 13000/32493 [36:53<54:58,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-13000\n",
            "Configuration saved in ./trained_model_5/checkpoint-13000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-13000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-13000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-13000/special_tokens_map.json\n",
            "{'loss': 0.3791, 'learning_rate': 2.9226294894284923e-05, 'epoch': 1.25}\n",
            " 42% 13500/32493 [38:18<53:23,  5.93it/s]Saving model checkpoint to ./trained_model_5/checkpoint-13500\n",
            "Configuration saved in ./trained_model_5/checkpoint-13500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-13500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-13500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-13500/special_tokens_map.json\n",
            "{'loss': 0.3852, 'learning_rate': 2.8456898408888072e-05, 'epoch': 1.29}\n",
            " 43% 14000/32493 [39:43<52:08,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-14000\n",
            "Configuration saved in ./trained_model_5/checkpoint-14000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-14000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-14000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-14000/special_tokens_map.json\n",
            "{'loss': 0.3814, 'learning_rate': 2.7687501923491215e-05, 'epoch': 1.34}\n",
            " 45% 14500/32493 [41:09<51:10,  5.86it/s]Saving model checkpoint to ./trained_model_5/checkpoint-14500\n",
            "Configuration saved in ./trained_model_5/checkpoint-14500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-14500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-14500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-14500/special_tokens_map.json\n",
            "{'loss': 0.3715, 'learning_rate': 2.691810543809436e-05, 'epoch': 1.38}\n",
            " 46% 15000/32493 [42:34<49:21,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-15000\n",
            "Configuration saved in ./trained_model_5/checkpoint-15000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-15000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-15000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-15000/special_tokens_map.json\n",
            "{'loss': 0.3843, 'learning_rate': 2.6148708952697503e-05, 'epoch': 1.43}\n",
            " 48% 15500/32493 [43:59<47:48,  5.92it/s]Saving model checkpoint to ./trained_model_5/checkpoint-15500\n",
            "Configuration saved in ./trained_model_5/checkpoint-15500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-15500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-15500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-15500/special_tokens_map.json\n",
            "{'loss': 0.3825, 'learning_rate': 2.5379312467300653e-05, 'epoch': 1.48}\n",
            " 49% 16000/32493 [45:24<46:34,  5.90it/s]Saving model checkpoint to ./trained_model_5/checkpoint-16000\n",
            "Configuration saved in ./trained_model_5/checkpoint-16000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-16000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-16000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-16000/special_tokens_map.json\n",
            "{'loss': 0.3784, 'learning_rate': 2.46099159819038e-05, 'epoch': 1.52}\n",
            " 51% 16500/32493 [46:49<45:16,  5.89it/s]Saving model checkpoint to ./trained_model_5/checkpoint-16500\n",
            "Configuration saved in ./trained_model_5/checkpoint-16500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-16500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-16500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-16500/special_tokens_map.json\n",
            "{'loss': 0.3764, 'learning_rate': 2.384051949650694e-05, 'epoch': 1.57}\n",
            " 52% 17000/32493 [48:14<43:46,  5.90it/s]Saving model checkpoint to ./trained_model_5/checkpoint-17000\n",
            "Configuration saved in ./trained_model_5/checkpoint-17000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-17000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-17000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-17000/special_tokens_map.json\n",
            "{'loss': 0.3742, 'learning_rate': 2.3071123011110084e-05, 'epoch': 1.62}\n",
            " 54% 17500/32493 [49:40<42:36,  5.87it/s]Saving model checkpoint to ./trained_model_5/checkpoint-17500\n",
            "Configuration saved in ./trained_model_5/checkpoint-17500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-17500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-17500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-17500/special_tokens_map.json\n",
            "{'loss': 0.3711, 'learning_rate': 2.2301726525713233e-05, 'epoch': 1.66}\n",
            " 55% 18000/32493 [51:05<40:45,  5.93it/s]Saving model checkpoint to ./trained_model_5/checkpoint-18000\n",
            "Configuration saved in ./trained_model_5/checkpoint-18000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-18000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-18000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-18000/special_tokens_map.json\n",
            "{'loss': 0.3669, 'learning_rate': 2.1532330040316375e-05, 'epoch': 1.71}\n",
            " 57% 18500/32493 [52:30<39:49,  5.86it/s]Saving model checkpoint to ./trained_model_5/checkpoint-18500\n",
            "Configuration saved in ./trained_model_5/checkpoint-18500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-18500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-18500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-18500/special_tokens_map.json\n",
            "{'loss': 0.3723, 'learning_rate': 2.076293355491952e-05, 'epoch': 1.75}\n",
            " 58% 19000/32493 [53:55<37:59,  5.92it/s]Saving model checkpoint to ./trained_model_5/checkpoint-19000\n",
            "Configuration saved in ./trained_model_5/checkpoint-19000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-19000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-19000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-19000/special_tokens_map.json\n",
            "{'loss': 0.369, 'learning_rate': 1.9993537069522667e-05, 'epoch': 1.8}\n",
            " 60% 19500/32493 [55:21<36:39,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-19500\n",
            "Configuration saved in ./trained_model_5/checkpoint-19500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-19500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-19500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-19500/special_tokens_map.json\n",
            "{'loss': 0.3663, 'learning_rate': 1.9224140584125813e-05, 'epoch': 1.85}\n",
            " 62% 20000/32493 [56:46<35:07,  5.93it/s]Saving model checkpoint to ./trained_model_5/checkpoint-20000\n",
            "Configuration saved in ./trained_model_5/checkpoint-20000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-20000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-20000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-20000/special_tokens_map.json\n",
            "{'loss': 0.3694, 'learning_rate': 1.8454744098728956e-05, 'epoch': 1.89}\n",
            " 63% 20500/32493 [58:11<33:51,  5.90it/s]Saving model checkpoint to ./trained_model_5/checkpoint-20500\n",
            "Configuration saved in ./trained_model_5/checkpoint-20500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-20500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-20500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-20500/special_tokens_map.json\n",
            "{'loss': 0.3638, 'learning_rate': 1.7685347613332105e-05, 'epoch': 1.94}\n",
            " 65% 21000/32493 [59:36<32:38,  5.87it/s]Saving model checkpoint to ./trained_model_5/checkpoint-21000\n",
            "Configuration saved in ./trained_model_5/checkpoint-21000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-21000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-21000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-21000/special_tokens_map.json\n",
            "{'loss': 0.3697, 'learning_rate': 1.6915951127935248e-05, 'epoch': 1.99}\n",
            " 66% 21500/32493 [1:01:01<30:56,  5.92it/s]Saving model checkpoint to ./trained_model_5/checkpoint-21500\n",
            "Configuration saved in ./trained_model_5/checkpoint-21500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-21500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-21500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-21500/special_tokens_map.json\n",
            "{'loss': 0.3418, 'learning_rate': 1.6146554642538394e-05, 'epoch': 2.03}\n",
            " 68% 22000/32493 [1:02:26<29:34,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-22000\n",
            "Configuration saved in ./trained_model_5/checkpoint-22000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-22000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-22000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-22000/special_tokens_map.json\n",
            "{'loss': 0.3299, 'learning_rate': 1.537715815714154e-05, 'epoch': 2.08}\n",
            " 69% 22500/32493 [1:03:52<28:03,  5.94it/s]Saving model checkpoint to ./trained_model_5/checkpoint-22500\n",
            "Configuration saved in ./trained_model_5/checkpoint-22500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-22500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-22500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-22500/special_tokens_map.json\n",
            "{'loss': 0.326, 'learning_rate': 1.4607761671744686e-05, 'epoch': 2.12}\n",
            " 71% 23000/32493 [1:05:17<26:43,  5.92it/s]Saving model checkpoint to ./trained_model_5/checkpoint-23000\n",
            "Configuration saved in ./trained_model_5/checkpoint-23000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-23000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-23000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-23000/special_tokens_map.json\n",
            "{'loss': 0.3218, 'learning_rate': 1.383836518634783e-05, 'epoch': 2.17}\n",
            " 72% 23500/32493 [1:06:42<25:20,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-23500\n",
            "Configuration saved in ./trained_model_5/checkpoint-23500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-23500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-23500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-23500/special_tokens_map.json\n",
            "{'loss': 0.3211, 'learning_rate': 1.3068968700950976e-05, 'epoch': 2.22}\n",
            " 74% 24000/32493 [1:08:07<23:59,  5.90it/s]Saving model checkpoint to ./trained_model_5/checkpoint-24000\n",
            "Configuration saved in ./trained_model_5/checkpoint-24000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-24000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-24000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-24000/special_tokens_map.json\n",
            "{'loss': 0.3227, 'learning_rate': 1.229957221555412e-05, 'epoch': 2.26}\n",
            " 75% 24500/32493 [1:09:32<22:31,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-24500\n",
            "Configuration saved in ./trained_model_5/checkpoint-24500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-24500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-24500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-24500/special_tokens_map.json\n",
            "{'loss': 0.3251, 'learning_rate': 1.1530175730157264e-05, 'epoch': 2.31}\n",
            " 77% 25000/32493 [1:10:58<20:59,  5.95it/s]Saving model checkpoint to ./trained_model_5/checkpoint-25000\n",
            "Configuration saved in ./trained_model_5/checkpoint-25000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-25000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-25000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-25000/special_tokens_map.json\n",
            "{'loss': 0.3258, 'learning_rate': 1.076077924476041e-05, 'epoch': 2.35}\n",
            " 78% 25500/32493 [1:12:23<19:45,  5.90it/s]Saving model checkpoint to ./trained_model_5/checkpoint-25500\n",
            "Configuration saved in ./trained_model_5/checkpoint-25500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-25500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-25500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-25500/special_tokens_map.json\n",
            "{'loss': 0.3213, 'learning_rate': 9.991382759363556e-06, 'epoch': 2.4}\n",
            " 80% 26000/32493 [1:13:48<18:20,  5.90it/s]Saving model checkpoint to ./trained_model_5/checkpoint-26000\n",
            "Configuration saved in ./trained_model_5/checkpoint-26000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-26000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-26000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-26000/special_tokens_map.json\n",
            "{'loss': 0.3181, 'learning_rate': 9.2219862739667e-06, 'epoch': 2.45}\n",
            " 82% 26500/32493 [1:15:13<16:52,  5.92it/s]Saving model checkpoint to ./trained_model_5/checkpoint-26500\n",
            "Configuration saved in ./trained_model_5/checkpoint-26500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-26500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-26500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-26500/special_tokens_map.json\n",
            "{'loss': 0.3208, 'learning_rate': 8.452589788569846e-06, 'epoch': 2.49}\n",
            " 83% 27000/32493 [1:16:39<15:33,  5.88it/s]Saving model checkpoint to ./trained_model_5/checkpoint-27000\n",
            "Configuration saved in ./trained_model_5/checkpoint-27000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-27000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-27000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-27000/special_tokens_map.json\n",
            "{'loss': 0.3192, 'learning_rate': 7.683193303172992e-06, 'epoch': 2.54}\n",
            " 85% 27500/32493 [1:18:04<14:10,  5.87it/s]Saving model checkpoint to ./trained_model_5/checkpoint-27500\n",
            "Configuration saved in ./trained_model_5/checkpoint-27500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-27500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-27500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-27500/special_tokens_map.json\n",
            "{'loss': 0.3211, 'learning_rate': 6.9137968177761375e-06, 'epoch': 2.59}\n",
            " 86% 28000/32493 [1:19:29<12:40,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-28000\n",
            "Configuration saved in ./trained_model_5/checkpoint-28000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-28000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-28000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-28000/special_tokens_map.json\n",
            "{'loss': 0.3211, 'learning_rate': 6.144400332379282e-06, 'epoch': 2.63}\n",
            " 88% 28500/32493 [1:20:54<11:16,  5.90it/s]Saving model checkpoint to ./trained_model_5/checkpoint-28500\n",
            "Configuration saved in ./trained_model_5/checkpoint-28500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-28500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-28500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-28500/special_tokens_map.json\n",
            "{'loss': 0.3291, 'learning_rate': 5.375003846982427e-06, 'epoch': 2.68}\n",
            " 89% 29000/32493 [1:22:19<09:54,  5.87it/s]Saving model checkpoint to ./trained_model_5/checkpoint-29000\n",
            "Configuration saved in ./trained_model_5/checkpoint-29000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-29000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-29000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-29000/special_tokens_map.json\n",
            "{'loss': 0.3195, 'learning_rate': 4.605607361585573e-06, 'epoch': 2.72}\n",
            " 91% 29500/32493 [1:23:45<08:32,  5.84it/s]Saving model checkpoint to ./trained_model_5/checkpoint-29500\n",
            "Configuration saved in ./trained_model_5/checkpoint-29500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-29500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-29500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-29500/special_tokens_map.json\n",
            "{'loss': 0.3193, 'learning_rate': 3.836210876188718e-06, 'epoch': 2.77}\n",
            " 92% 30000/32493 [1:25:10<07:02,  5.90it/s]Saving model checkpoint to ./trained_model_5/checkpoint-30000\n",
            "Configuration saved in ./trained_model_5/checkpoint-30000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-30000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-30000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-30000/special_tokens_map.json\n",
            "{'loss': 0.3185, 'learning_rate': 3.066814390791863e-06, 'epoch': 2.82}\n",
            " 94% 30500/32493 [1:26:35<05:36,  5.92it/s]Saving model checkpoint to ./trained_model_5/checkpoint-30500\n",
            "Configuration saved in ./trained_model_5/checkpoint-30500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-30500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-30500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-30500/special_tokens_map.json\n",
            "{'loss': 0.3152, 'learning_rate': 2.297417905395008e-06, 'epoch': 2.86}\n",
            " 95% 31000/32493 [1:28:00<04:12,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-31000\n",
            "Configuration saved in ./trained_model_5/checkpoint-31000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-31000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-31000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-31000/special_tokens_map.json\n",
            "{'loss': 0.3154, 'learning_rate': 1.5280214199981535e-06, 'epoch': 2.91}\n",
            " 97% 31500/32493 [1:29:26<02:47,  5.92it/s]Saving model checkpoint to ./trained_model_5/checkpoint-31500\n",
            "Configuration saved in ./trained_model_5/checkpoint-31500/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-31500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-31500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-31500/special_tokens_map.json\n",
            "{'loss': 0.314, 'learning_rate': 7.586249346012988e-07, 'epoch': 2.95}\n",
            " 98% 32000/32493 [1:30:51<01:23,  5.91it/s]Saving model checkpoint to ./trained_model_5/checkpoint-32000\n",
            "Configuration saved in ./trained_model_5/checkpoint-32000/config.json\n",
            "Model weights saved in ./trained_model_5/checkpoint-32000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/checkpoint-32000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/checkpoint-32000/special_tokens_map.json\n",
            "100% 32492/32493 [1:32:15<00:00,  5.88it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 5535.1504, 'train_samples_per_second': 352.2, 'train_steps_per_second': 5.87, 'train_loss': 0.3960212113707363, 'epoch': 3.0}\n",
            "100% 32493/32493 [1:32:15<00:00,  5.87it/s]\n",
            "Saving model checkpoint to ./trained_model_5/\n",
            "Configuration saved in ./trained_model_5/config.json\n",
            "Model weights saved in ./trained_model_5/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_5/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_5/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNtJ2T8SemLP",
        "outputId": "34c333fc-d9be-4907-95c1-e26f091d416e"
      },
      "source": [
        "#eval on anli[dev_r1]\n",
        "!python3 run-conc.py --do_eval --task nli --dataset anli --eval_split dev_r1 --concatenate none --model ./trained_model_2/ --output_dir ./eval_output_2a/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reusing dataset anli (/root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b)\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-dee798ba540283a0.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-58ba46ef8e5b94d9.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-463cea97051aab54.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-ab033209f9ad42cb.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-1233a8a1c20a12a3.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-8524e79e437f66fd.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-2bfb9f21138d07c7.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-eeb5c0df588c2d46.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-eba49f9ca4f71617.arrow\n",
            " #0:   0% 0/1 [00:00<?, ?ba/s]\n",
            " #1:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\n",
            " #1: 100% 1/1 [00:00<00:00,  4.33ba/s]\n",
            " #0: 100% 1/1 [00:00<00:00,  4.19ba/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "100% 125/125 [00:01<00:00, 63.15it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 1.4676775932312012, 'eval_accuracy': 0.4650000035762787, 'eval_runtime': 2.0111, 'eval_samples_per_second': 497.243, 'eval_steps_per_second': 62.155}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tpmgd-EU3EA",
        "outputId": "99b31755-a97a-44b7-df65-56f5645548c1"
      },
      "source": [
        "#eval on anli[dev_r3]\n",
        "!python3 run-conc.py --do_eval --task nli --dataset anli --concatenate none --eval_split dev_r3 --model ./trained_model_2/ --output_dir ./eval_output_2b/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reusing dataset anli (/root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b)\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-dee798ba540283a0.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-58ba46ef8e5b94d9.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-463cea97051aab54.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-ab033209f9ad42cb.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-1233a8a1c20a12a3.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-8524e79e437f66fd.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-2bfb9f21138d07c7.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-eeb5c0df588c2d46.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b/cache-eba49f9ca4f71617.arrow\n",
            " #0:   0% 0/1 [00:00<?, ?ba/s]\n",
            " #1:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\n",
            " #1: 100% 1/1 [00:00<00:00,  3.88ba/s]\n",
            " #0: 100% 1/1 [00:00<00:00,  3.83ba/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1200\n",
            "  Batch size = 8\n",
            "100% 150/150 [00:02<00:00, 65.33it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 1.581135869026184, 'eval_accuracy': 0.4258333444595337, 'eval_runtime': 2.3192, 'eval_samples_per_second': 517.421, 'eval_steps_per_second': 64.678}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLTnV1l2U3Z4",
        "outputId": "2907af8b-159c-4fed-95e7-7c32f10b2982"
      },
      "source": [
        "#eval on snli\n",
        "!python3 run-conc.py --do_eval --task nli --dataset snli --concatenate none --model ./trained_model_2/ --output_dir ./eval_output_2c/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reusing dataset snli (/root/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-be8c311388e5d6eb.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-39e7bac6fcfd8931.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-d228f431068e685f.arrow\n",
            " #0:   0% 0/5 [00:00<?, ?ba/s]\n",
            " #0:  20% 1/5 [00:00<00:00,  4.30ba/s]\n",
            " #0:  40% 2/5 [00:00<00:00,  5.05ba/s]\n",
            " #0:  60% 3/5 [00:00<00:00,  5.15ba/s]\n",
            " #0:  80% 4/5 [00:00<00:00,  5.20ba/s]\n",
            " #0: 100% 5/5 [00:00<00:00,  5.24ba/s]\n",
            "\n",
            " #1: 100% 5/5 [00:00<00:00,  5.23ba/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 9842\n",
            "  Batch size = 8\n",
            "100% 1231/1231 [00:20<00:00, 60.86it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 0.29554328322410583, 'eval_accuracy': 0.8971753716468811, 'eval_runtime': 20.2469, 'eval_samples_per_second': 486.099, 'eval_steps_per_second': 60.799}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuvD6nnwVTJU",
        "outputId": "9bcac198-595b-4500-ea72-54fce32d1053"
      },
      "source": [
        "#eval on pelz_data\n",
        "!python3 run-conc.py --do_eval --task nli --dataset pelz_data --concatenate none --model ./trained_model_2/ --output_dir ./eval_output_2d/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-3e50e61b02b0703b\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-3e50e61b02b0703b/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n",
            "\r0 tables [00:00, ? tables/s]\r                            \rDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-3e50e61b02b0703b/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            " #0:   0% 0/1 [00:00<?, ?ba/s]\n",
            " #0: 100% 1/1 [00:00<00:00, 48.78ba/s]\n",
            " #1: 100% 1/1 [00:00<00:00, 55.56ba/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 51\n",
            "  Batch size = 8\n",
            "100% 7/7 [00:00<00:00, 69.98it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 0.50764399766922, 'eval_accuracy': 0.7843137383460999, 'eval_runtime': 0.1241, 'eval_samples_per_second': 411.081, 'eval_steps_per_second': 56.423}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zebAhl_Md8AU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e34f94-d960-42d3-da6d-339b35d3da16"
      },
      "source": [
        "#trained on snli + anli[train_r3]\n",
        "!python3 run-conc.py --do_train --task nli --dataset anli --concatenate none --per_device_train_batch_size 60 --output_dir ./trained_model_4/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 5.55kB [00:00, 6.62MB/s]       \n",
            "Downloading: 2.76kB [00:00, 3.46MB/s]       \n",
            "Downloading and preparing dataset anli/plain_text (download: 17.76 MiB, generated: 73.55 MiB, post-processed: Unknown size, total: 91.31 MiB) to /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b...\n",
            "Downloading: 100% 18.6M/18.6M [00:02<00:00, 7.59MB/s]\n",
            "Dataset anli downloaded and prepared to /root/.cache/huggingface/datasets/anli/plain_text/0.1.0/aabce88453b06dff21c201855ea83283bab0390bff746deadb30b65695755c0b. Subsequent calls will reuse this data.\n",
            "Downloading: 100% 665/665 [00:00<00:00, 643kB/s]\n",
            "Downloading: 100% 54.2M/54.2M [00:00<00:00, 56.8MB/s]\n",
            "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 27.2kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 320kB/s]\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 513kB/s]\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "100% 17/17 [00:00<00:00, 34.52ba/s]\n",
            "100% 1/1 [00:00<00:00, 31.56ba/s]\n",
            "100% 1/1 [00:00<00:00, 30.65ba/s]\n",
            "100% 46/46 [00:01<00:00, 34.32ba/s]\n",
            "100% 1/1 [00:00<00:00, 30.86ba/s]\n",
            "100% 1/1 [00:00<00:00, 32.64ba/s]\n",
            "100% 101/101 [00:02<00:00, 34.25ba/s]\n",
            "100% 2/2 [00:00<00:00, 52.89ba/s]\n",
            "100% 2/2 [00:00<00:00, 56.62ba/s]\n",
            " #0:   0% 0/51 [00:00<?, ?ba/s]\n",
            " #1:   0% 0/51 [00:00<?, ?ba/s]\u001b[A\n",
            " #0:   2% 1/51 [00:00<00:25,  1.94ba/s]\n",
            " #0:   4% 2/51 [00:00<00:21,  2.26ba/s]\n",
            " #0:   8% 4/51 [00:01<00:18,  2.50ba/s]\n",
            " #1:   8% 4/51 [00:01<00:19,  2.40ba/s]\u001b[A\n",
            " #0:  12% 6/51 [00:02<00:18,  2.49ba/s]\n",
            " #0:  14% 7/51 [00:02<00:17,  2.54ba/s]\n",
            " #0:  16% 8/51 [00:03<00:16,  2.59ba/s]\n",
            " #0:  18% 9/51 [00:03<00:16,  2.59ba/s]\n",
            " #0:  20% 10/51 [00:04<00:15,  2.56ba/s]\n",
            " #0:  22% 11/51 [00:04<00:15,  2.58ba/s]\n",
            " #0:  24% 12/51 [00:04<00:15,  2.57ba/s]\n",
            " #0:  25% 13/51 [00:05<00:15,  2.53ba/s]\n",
            " #0:  27% 14/51 [00:05<00:14,  2.55ba/s]\n",
            " #0:  29% 15/51 [00:05<00:13,  2.60ba/s]\n",
            " #0:  31% 16/51 [00:06<00:13,  2.62ba/s]\n",
            " #0:  33% 17/51 [00:06<00:12,  2.62ba/s]\n",
            " #0:  35% 18/51 [00:07<00:12,  2.59ba/s]\n",
            " #0:  37% 19/51 [00:07<00:14,  2.24ba/s]\n",
            " #0:  41% 21/51 [00:08<00:12,  2.46ba/s]\n",
            " #0:  43% 22/51 [00:08<00:11,  2.51ba/s]\n",
            " #0:  45% 23/51 [00:09<00:11,  2.54ba/s]\n",
            " #0:  47% 24/51 [00:09<00:10,  2.56ba/s]\n",
            " #0:  49% 25/51 [00:09<00:10,  2.57ba/s]\n",
            " #0:  51% 26/51 [00:10<00:09,  2.59ba/s]\n",
            " #0:  53% 27/51 [00:10<00:09,  2.61ba/s]\n",
            " #0:  55% 28/51 [00:11<00:08,  2.56ba/s]\n",
            " #0:  57% 29/51 [00:11<00:08,  2.58ba/s]\n",
            " #0:  59% 30/51 [00:11<00:08,  2.59ba/s]\n",
            " #0:  61% 31/51 [00:12<00:07,  2.54ba/s]\n",
            " #0:  63% 32/51 [00:12<00:07,  2.53ba/s]\n",
            " #0:  65% 33/51 [00:13<00:07,  2.52ba/s]\n",
            " #0:  67% 34/51 [00:13<00:06,  2.54ba/s]\n",
            " #0:  69% 35/51 [00:13<00:06,  2.55ba/s]\n",
            " #0:  71% 36/51 [00:14<00:05,  2.53ba/s]\n",
            " #0:  73% 37/51 [00:14<00:05,  2.53ba/s]\n",
            " #0:  75% 38/51 [00:15<00:05,  2.54ba/s]\n",
            " #0:  76% 39/51 [00:15<00:04,  2.57ba/s]\n",
            " #0:  78% 40/51 [00:15<00:04,  2.55ba/s]\n",
            " #0:  80% 41/51 [00:16<00:03,  2.54ba/s]\n",
            " #0:  82% 42/51 [00:16<00:03,  2.54ba/s]\n",
            " #0:  84% 43/51 [00:17<00:03,  2.54ba/s]\n",
            " #0:  86% 44/51 [00:17<00:02,  2.56ba/s]\n",
            " #0:  88% 45/51 [00:17<00:02,  2.58ba/s]\n",
            " #0:  90% 46/51 [00:18<00:01,  2.57ba/s]\n",
            " #0:  92% 47/51 [00:18<00:01,  2.57ba/s]\n",
            " #1:  90% 46/51 [00:18<00:01,  2.80ba/s]\u001b[A\n",
            " #0:  94% 48/51 [00:18<00:01,  2.58ba/s]\n",
            " #0:  96% 49/51 [00:19<00:00,  2.56ba/s]\n",
            " #0: 100% 51/51 [00:19<00:00,  2.57ba/s]\n",
            "\n",
            " #1: 100% 51/51 [00:19<00:00,  2.56ba/s]\n",
            "***** Running training *****\n",
            "  Num examples = 100459\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 60\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 60\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 5025\n",
            "{'loss': 0.935, 'learning_rate': 4.502487562189055e-05, 'epoch': 0.3}\n",
            " 10% 500/5025 [01:24<12:42,  5.94it/s]Saving model checkpoint to ./trained_model_4/checkpoint-500\n",
            "Configuration saved in ./trained_model_4/checkpoint-500/config.json\n",
            "Model weights saved in ./trained_model_4/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_4/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_4/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.7512, 'learning_rate': 4.00497512437811e-05, 'epoch': 0.6}\n",
            " 20% 1000/5025 [02:49<11:18,  5.93it/s]Saving model checkpoint to ./trained_model_4/checkpoint-1000\n",
            "Configuration saved in ./trained_model_4/checkpoint-1000/config.json\n",
            "Model weights saved in ./trained_model_4/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_4/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_4/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.6921, 'learning_rate': 3.5074626865671645e-05, 'epoch': 0.9}\n",
            " 30% 1500/5025 [04:14<09:55,  5.92it/s]Saving model checkpoint to ./trained_model_4/checkpoint-1500\n",
            "Configuration saved in ./trained_model_4/checkpoint-1500/config.json\n",
            "Model weights saved in ./trained_model_4/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_4/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_4/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 0.6257, 'learning_rate': 3.009950248756219e-05, 'epoch': 1.19}\n",
            " 40% 2000/5025 [05:39<08:30,  5.93it/s]Saving model checkpoint to ./trained_model_4/checkpoint-2000\n",
            "Configuration saved in ./trained_model_4/checkpoint-2000/config.json\n",
            "Model weights saved in ./trained_model_4/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_4/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_4/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 0.5758, 'learning_rate': 2.512437810945274e-05, 'epoch': 1.49}\n",
            " 50% 2500/5025 [07:04<07:06,  5.93it/s]Saving model checkpoint to ./trained_model_4/checkpoint-2500\n",
            "Configuration saved in ./trained_model_4/checkpoint-2500/config.json\n",
            "Model weights saved in ./trained_model_4/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_4/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_4/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 0.5767, 'learning_rate': 2.0149253731343285e-05, 'epoch': 1.79}\n",
            " 60% 3000/5025 [08:29<05:41,  5.92it/s]Saving model checkpoint to ./trained_model_4/checkpoint-3000\n",
            "Configuration saved in ./trained_model_4/checkpoint-3000/config.json\n",
            "Model weights saved in ./trained_model_4/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_4/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_4/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 0.5307, 'learning_rate': 1.5174129353233832e-05, 'epoch': 2.09}\n",
            " 70% 3500/5025 [09:54<04:18,  5.89it/s]Saving model checkpoint to ./trained_model_4/checkpoint-3500\n",
            "Configuration saved in ./trained_model_4/checkpoint-3500/config.json\n",
            "Model weights saved in ./trained_model_4/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_4/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_4/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 0.4822, 'learning_rate': 1.0199004975124378e-05, 'epoch': 2.39}\n",
            " 80% 4000/5025 [11:19<02:52,  5.93it/s]Saving model checkpoint to ./trained_model_4/checkpoint-4000\n",
            "Configuration saved in ./trained_model_4/checkpoint-4000/config.json\n",
            "Model weights saved in ./trained_model_4/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_4/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_4/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 0.4822, 'learning_rate': 5.2238805970149255e-06, 'epoch': 2.69}\n",
            " 90% 4500/5025 [12:44<01:28,  5.94it/s]Saving model checkpoint to ./trained_model_4/checkpoint-4500\n",
            "Configuration saved in ./trained_model_4/checkpoint-4500/config.json\n",
            "Model weights saved in ./trained_model_4/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_4/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_4/checkpoint-4500/special_tokens_map.json\n",
            "{'loss': 0.48, 'learning_rate': 2.4875621890547267e-07, 'epoch': 2.99}\n",
            "100% 5000/5025 [14:09<00:04,  5.93it/s]Saving model checkpoint to ./trained_model_4/checkpoint-5000\n",
            "Configuration saved in ./trained_model_4/checkpoint-5000/config.json\n",
            "Model weights saved in ./trained_model_4/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_4/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_4/checkpoint-5000/special_tokens_map.json\n",
            "100% 5024/5025 [14:14<00:00,  5.89it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 854.3648, 'train_samples_per_second': 352.75, 'train_steps_per_second': 5.882, 'train_loss': 0.6124767559677807, 'epoch': 3.0}\n",
            "100% 5025/5025 [14:14<00:00,  5.88it/s]\n",
            "Saving model checkpoint to ./trained_model_4/\n",
            "Configuration saved in ./trained_model_4/config.json\n",
            "Model weights saved in ./trained_model_4/pytorch_model.bin\n",
            "tokenizer config file saved in ./trained_model_4/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_model_4/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96svR-PiY9q9",
        "outputId": "03116287-59af-4a68-ec03-057f8c4a5b20"
      },
      "source": [
        "#eval on pelz_data\n",
        "!python3 run-conc.py --do_eval --task nli --dataset pelz_data --concatenate none --model ./trained_model_5/ --output_dir ./eval_output_4e/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using custom data configuration default-2b9d5a0086adce2b\n",
            "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-2b9d5a0086adce2b/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            " #0:   0% 0/1 [00:00<?, ?ba/s]\n",
            " #0: 100% 1/1 [00:00<00:00, 41.96ba/s]\n",
            " #1: 100% 1/1 [00:00<00:00, 50.79ba/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 9\n",
            "  Batch size = 8\n",
            "100% 2/2 [00:00<00:00, 96.81it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 1.1000454425811768, 'eval_accuracy': 0.5555555820465088, 'eval_runtime': 0.056, 'eval_samples_per_second': 160.762, 'eval_steps_per_second': 35.725}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHJMZd50Z0L9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}